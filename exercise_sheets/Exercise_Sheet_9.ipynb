{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language Processing\n",
    "\n",
    "## Exercise Sheet 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports for all exercises\n",
    "import nltk\n",
    "from nltk import grammar, parse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1\n",
    "\n",
    "Take the following grammar:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = \"\"\"\n",
    "%start S\n",
    "S                    -> NP[AGR=?n] VP[AGR=?n]\n",
    "NP[AGR=?n]           -> PropN[AGR=?n]\n",
    "VP[TENSE=?t, AGR=?n] -> Cop[TENSE=?t, AGR=?n] Adj\n",
    "\n",
    "\n",
    "Cop[TENSE=pres,  AGR=[NUM=sg, PER=1]] -> 'am'\n",
    "Cop[TENSE=pres,  AGR=[NUM=sg, PER=2]] -> 'are'\n",
    "Cop[TENSE=pres,  AGR=[NUM=sg, PER=3]] -> 'is'\n",
    "PropN[AGR=[NUM=sg, PER=1]]            -> 'I'\n",
    "PropN[AGR=[NUM=sg, PER=2]]            -> 'you'\n",
    "PropN[AGR=[NUM=sg, PER=3]]            -> 'she'\n",
    "PropN[AGR=[NUM=pl, PER=3]]            -> 'they'\n",
    "Adj                                   -> 'happy'\n",
    "\"\"\"\n",
    "gr = grammar.FeatureGrammar.fromstring(g)\n",
    "\n",
    "def parse_sent(sent, gr):\n",
    "    tokens = sent.split()\n",
    "    parser = parse.FeatureEarleyChartParser(gr)\n",
    "    trees = parser.parse(tokens)\n",
    "    for tree in trees: print(tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "as starting point to correctly parse word sequences like \"I am happy\" and \"she is happy\" but not \" * you is happy\" or \" * they am happy\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent1 = \"I am happy\"\n",
    "sent2 = \"she is happy\"\n",
    "sent3 = \"you is happy\"\n",
    "sent4 = \"they am happy\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S[]\n",
      "  (NP[AGR=[NUM='sg', PER=1]] (PropN[AGR=[NUM='sg', PER=1]] I))\n",
      "  (VP[AGR=[NUM='sg', PER=1], TENSE='pres']\n",
      "    (Cop[AGR=[NUM='sg', PER=1], TENSE='pres'] am)\n",
      "    (Adj[] happy)))\n"
     ]
    }
   ],
   "source": [
    "parse_sent(sent1,gr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S[]\n",
      "  (NP[AGR=[NUM='sg', PER=3]] (PropN[AGR=[NUM='sg', PER=3]] she))\n",
      "  (VP[AGR=[NUM='sg', PER=3], TENSE='pres']\n",
      "    (Cop[AGR=[NUM='sg', PER=3], TENSE='pres'] is)\n",
      "    (Adj[] happy)))\n"
     ]
    }
   ],
   "source": [
    "parse_sent(sent2,gr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "parse_sent(sent3,gr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "parse_sent(sent4,gr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2\n",
    "\n",
    "Develop a variant of the following grammar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = \"\"\"\n",
    "% start S\n",
    "\n",
    "# ###################\n",
    "# Grammar Productions\n",
    "# ###################\n",
    "# S expansion productions\n",
    "S -> NP[NUM=?n] VP[NUM=?n]\n",
    "# NP expansion productions\n",
    "NP[NUM=?n] -> N[NUM=?n, -count]\n",
    "NP[NUM=?n] -> PropN[NUM=?n]\n",
    "NP[NUM=?n] -> Det[NUM=?n] N[NUM=?n]\n",
    "NP[NUM=pl] -> N[NUM=pl]\n",
    "# VP expansion productions\n",
    "VP[TENSE=?t, NUM=?n] -> IV[TENSE=?t, NUM=?n]\n",
    "VP[TENSE=?t, NUM=?n] -> TV[TENSE=?t, NUM=?n] NP\n",
    "VP[TENSE=?t, AGR=?n] -> Cop[TENSE=?t, AGR=?n] Adj\n",
    "# ###################\n",
    "# Lexical Productions\n",
    "# ###################\n",
    "Det[NUM=sg] -> 'this' | 'every'\n",
    "Det[NUM=pl] -> 'these' | 'all'\n",
    "Det -> 'the' | 'some' | 'several'\n",
    "PropN[NUM=sg]-> 'Kim' | 'Jody'\n",
    "N[NUM=sg, +count] -> 'dog' | 'girl' | 'car' | 'child' | 'boy' | 'water'\n",
    "N[NUM=pl, +count] -> 'dogs' | 'girls' | 'cars' | 'children' |'boys'\n",
    "N[NUM=sg, -count] -> 'water'\n",
    "IV[TENSE=pres,  NUM=sg] -> 'disappears' | 'walks' | 'sings'\n",
    "TV[TENSE=pres, NUM=sg] -> 'sees' | 'likes' \n",
    "IV[TENSE=pres,  NUM=pl] -> 'disappear' | 'walk' | 'sing'\n",
    "TV[TENSE=pres, NUM=pl] -> 'see' | 'like'\n",
    "IV[TENSE=past] -> 'disappeared' | 'walked'\n",
    "TV[TENSE=past] -> 'saw' | 'liked'\n",
    "Cop[TENSE=pres,  AGR=[NUM=sg, PER=3]] -> 'is'\n",
    "Adj                                   -> 'precious'\n",
    "\"\"\"\n",
    "\n",
    "gr = grammar.FeatureGrammar.fromstring(g)\n",
    "\n",
    "def parse_sent(sent, gr):\n",
    "    tokens = sent.split()\n",
    "    parser = parse.FeatureEarleyChartParser(gr)\n",
    "    trees = parser.parse(tokens)\n",
    "    for tree in trees: print(tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "that uses a feature `COUNT` to make the distinctions shown below:\n",
    "\n",
    "(1a) the boy sings\n",
    "\n",
    "(1b) * boy sings\n",
    "\n",
    "(2a) the boys sing\n",
    "\n",
    "(2b) boys sing\n",
    "\n",
    "(3a) the water is precious\n",
    "\n",
    "(3b) water is precious"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent1 = \"the boy sings\"\n",
    "sent2 = \"boy sings\"\n",
    "sent3 = \"the boys sing\"\n",
    "sent4 = \"boys sing\"\n",
    "sent5 = \"the water is precious\"\n",
    "sent6 = \"water is precious\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S[]\n",
      "  (NP[NUM='sg'] (N[NUM='sg', -count] water))\n",
      "  (VP[AGR=[NUM='sg', PER=3], TENSE='pres']\n",
      "    (Cop[AGR=[NUM='sg', PER=3], TENSE='pres'] is)\n",
      "    (Adj[] precious)))\n"
     ]
    }
   ],
   "source": [
    "parse_sent(sent6,gr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3\n",
    "\n",
    "Extend the German grammar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = \"\"\"\n",
    "% start S\n",
    "# Grammar Productions\n",
    " S -> NP[CASE=nom, AGR=?a] VP[AGR=?a]\n",
    " S/TV -> NP \n",
    " NP[CASE=?c, AGR=?a] -> PRO[CASE=?c, AGR=?a]\n",
    " NP[CASE=?c, AGR=?a] -> Det[CASE=?c, AGR=?a] N[CASE=?c, AGR=?a]\n",
    " VP[AGR=?a] -> IV[AGR=?a]\n",
    " VP[AGR=?a] -> TV[OBJCASE=?c, AGR=?a] NP[CASE=?c]\n",
    " # Lexical Productions\n",
    " # Singular determiners\n",
    " # masc\n",
    " Det[CASE=nom, AGR=[GND=masc,PER=3,NUM=sg]] -> 'der'\n",
    " Det[CASE=dat, AGR=[GND=masc,PER=3,NUM=sg]] -> 'dem'\n",
    " Det[CASE=acc, AGR=[GND=masc,PER=3,NUM=sg]] -> 'den'\n",
    " # fem\n",
    " Det[CASE=nom, AGR=[GND=fem,PER=3,NUM=sg]] -> 'die'\n",
    " Det[CASE=dat, AGR=[GND=fem,PER=3,NUM=sg]] -> 'der'\n",
    " Det[CASE=acc, AGR=[GND=fem,PER=3,NUM=sg]] -> 'die'\n",
    " # Plural determiners\n",
    " Det[CASE=nom, AGR=[PER=3,NUM=pl]] -> 'die'\n",
    " Det[CASE=dat, AGR=[PER=3,NUM=pl]] -> 'den'\n",
    " Det[CASE=acc, AGR=[PER=3,NUM=pl]] -> 'die'\n",
    " # Nouns\n",
    " N[AGR=[GND=masc,PER=3,NUM=sg]] -> 'Hund'\n",
    " N[CASE=nom, AGR=[GND=masc,PER=3,NUM=pl]] -> 'Hunde'\n",
    " N[CASE=dat, AGR=[GND=masc,PER=3,NUM=pl]] -> 'Hunden'\n",
    " N[CASE=acc, AGR=[GND=masc,PER=3,NUM=pl]] -> 'Hunde'\n",
    " N[AGR=[GND=fem,PER=3,NUM=sg]] -> 'Katze'\n",
    " N[AGR=[GND=fem,PER=3,NUM=pl]] -> 'Katzen'\n",
    " # Pronouns\n",
    " PRO[CASE=nom, AGR=[PER=1,NUM=sg]] -> 'ich'\n",
    " PRO[CASE=acc, AGR=[PER=1,NUM=sg]] -> 'mich'\n",
    " PRO[CASE=dat, AGR=[PER=1,NUM=sg]] -> 'mir'\n",
    " PRO[CASE=nom, AGR=[PER=2,NUM=sg]] -> 'du'\n",
    " PRO[CASE=nom, AGR=[PER=3,NUM=sg]] -> 'er' | 'sie' | 'es'\n",
    " PRO[CASE=nom, AGR=[PER=1,NUM=pl]] -> 'wir'\n",
    " PRO[CASE=acc, AGR=[PER=1,NUM=pl]] -> 'uns'\n",
    " PRO[CASE=dat, AGR=[PER=1,NUM=pl]] -> 'uns'\n",
    " PRO[CASE=nom, AGR=[PER=2,NUM=pl]] -> 'ihr'\n",
    " PRO[CASE=nom, AGR=[PER=3,NUM=pl]] -> 'sie'\n",
    " # Verbs\n",
    " IV[AGR=[NUM=sg,PER=1]] -> 'komme'\n",
    " IV[AGR=[NUM=sg,PER=2]] -> 'kommst'\n",
    " IV[AGR=[NUM=sg,PER=3]] -> 'kommt'\n",
    " IV[AGR=[NUM=pl, PER=1]] -> 'kommen'\n",
    " IV[AGR=[NUM=pl, PER=2]] -> 'kommt'\n",
    " IV[AGR=[NUM=pl, PER=3]] -> 'kommen'\n",
    " TV[OBJCASE=acc, AGR=[NUM=sg,PER=1]] -> 'sehe' | 'mag'\n",
    " TV[OBJCASE=acc, AGR=[NUM=sg,PER=2]] -> 'siehst' | 'magst'\n",
    " TV[OBJCASE=acc, AGR=[NUM=sg,PER=3]] -> 'sieht' | 'mag'\n",
    " TV[OBJCASE=dat, AGR=[NUM=sg,PER=1]] -> 'folge' | 'helfe'\n",
    " TV[OBJCASE=dat, AGR=[NUM=sg,PER=2]] -> 'folgst' | 'hilfst'\n",
    " TV[OBJCASE=dat, AGR=[NUM=sg,PER=3]] -> 'folgt' | 'hilft'\n",
    " TV[OBJCASE=acc, AGR=[NUM=pl,PER=1]] -> 'sehen' | 'moegen'\n",
    " TV[OBJCASE=acc, AGR=[NUM=pl,PER=2]] -> 'sieht' | 'moegt'\n",
    " TV[OBJCASE=acc, AGR=[NUM=pl,PER=3]] -> 'sehen' | 'moegen'\n",
    " TV[OBJCASE=dat, AGR=[NUM=pl,PER=1]] -> 'folgen' | 'helfen'\n",
    " TV[OBJCASE=dat, AGR=[NUM=pl,PER=2]] -> 'folgt' | 'helft'\n",
    " TV[OBJCASE=dat, AGR=[NUM=pl,PER=3]] -> 'folgen' | 'helfen'\n",
    "\"\"\"\n",
    "gr = grammar.FeatureGrammar.fromstring(g)\n",
    "\n",
    "def parse_sent(sent, gr):\n",
    "    tokens = sent.split()\n",
    "    parser = parse.FeatureEarleyChartParser(gr)\n",
    "    trees = parser.parse(tokens)\n",
    "    for tree in trees: print(tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "so that it can handle so-called verb-second structures like \"heute sieht der Hund die Katze\" by using a slash category `S/TV` for the missing transitive verb in \"der Hund die Katze\". Use the following test sentences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent1 = \"heute sieht der Hund die Katze\"\n",
    "sent2 = \"heute sehe der Hund die Katze\"\n",
    "sent3 = \"heute sieht der Hund die Katzen\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Grammar does not cover some of the input words: \"'heute'\".",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_19352/505865944.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mparse_sent\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msent1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mgr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_19352/3327570212.py\u001b[0m in \u001b[0;36mparse_sent\u001b[1;34m(sent, gr)\u001b[0m\n\u001b[0;32m     64\u001b[0m     \u001b[0mtokens\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparse\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mFeatureEarleyChartParser\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 66\u001b[1;33m     \u001b[0mtrees\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     67\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mtree\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtrees\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtree\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\nltk\\parse\\chart.py\u001b[0m in \u001b[0;36mparse\u001b[1;34m(self, tokens, tree_class)\u001b[0m\n\u001b[0;32m   1472\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1473\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mparse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtokens\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtree_class\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mTree\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1474\u001b[1;33m         \u001b[0mchart\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchart_parse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1475\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0miter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mchart\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparses\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_grammar\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtree_class\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtree_class\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1476\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\nltk\\parse\\earleychart.py\u001b[0m in \u001b[0;36mchart_parse\u001b[1;34m(self, tokens, trace)\u001b[0m\n\u001b[0;32m    350\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    351\u001b[0m         \u001b[0mtokens\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 352\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_grammar\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcheck_coverage\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    353\u001b[0m         \u001b[0mchart\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_chart_class\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    354\u001b[0m         \u001b[0mgrammar\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_grammar\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\nltk\\grammar.py\u001b[0m in \u001b[0;36mcheck_coverage\u001b[1;34m(self, tokens)\u001b[0m\n\u001b[0;32m    662\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mmissing\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    663\u001b[0m             \u001b[0mmissing\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\", \"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"{w!r}\"\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmissing\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 664\u001b[1;33m             raise ValueError(\n\u001b[0m\u001b[0;32m    665\u001b[0m                 \u001b[1;34m\"Grammar does not cover some of the \"\u001b[0m \u001b[1;34m\"input words: %r.\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mmissing\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    666\u001b[0m             )\n",
      "\u001b[1;31mValueError\u001b[0m: Grammar does not cover some of the input words: \"'heute'\"."
     ]
    }
   ],
   "source": [
    "parse_sent(sent1,gr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4\n",
    "\n",
    "Consider the patterns of grammaticality for the verbs \"loaded\", \"filled\", and \"dumped\" below. Write grammar productions to handle such data:\n",
    "\n",
    "(1a) the farmer loaded the cart with sand\n",
    "\n",
    "(1b) the farmer loaded sand into the cart\n",
    "\n",
    "(2a) the farmer filled the cart with sand\n",
    "\n",
    "(2b) * the farmer filled sand into the cart\n",
    "\n",
    "(3a) * the farmer dumped the cart with sand\n",
    "\n",
    "(3b) the farmer dumped sand into the cart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "g = \"\"\"\n",
    "% start S\n",
    "# ###################\n",
    "# Grammar Productions\n",
    "# ###################\n",
    "# S expansion productions\n",
    "S -> NP[NUM=?n] VP[NUM=?n]\n",
    "# NP expansion productions\n",
    "NP[NUM=?n] -> N[NUM=?n]\n",
    "NP[NUM=?n] -> PropN[NUM=?n]\n",
    "NP[NUM=?n] -> Det[NUM=?n] N[NUM=?n]\n",
    "NP[NUM=pl] -> N[NUM=pl]\n",
    "# VP expansion productions\n",
    "VP[TENSE=?t, NUM=?n] -> IV[TENSE=?t, NUM=?n]\n",
    "VP[TENSE=?t, NUM=?n] -> TV[TENSE=?t, NUM=?n] NP\n",
    "VP[TENSE=?t, AGR=?n] -> Cop[TENSE=?t, AGR=?n] Adj\n",
    "# ###################\n",
    "# Lexical Productions\n",
    "# ###################\n",
    "Det[NUM=sg] -> 'this' | 'every'\n",
    "Det[NUM=pl] -> 'these' | 'all'\n",
    "Det -> 'the' | 'some' | 'several'\n",
    "PropN[NUM=sg]-> 'cart' | 'sand'\n",
    "N[NUM=sg] -> 'farmer'\n",
    "IV[TENSE=past] -> 'loaded' | 'filled' | 'dumped'\n",
    "\"\"\"\n",
    "gr = grammar.FeatureGrammar.fromstring(g)\n",
    "\n",
    "def parse_sent(sent, gr):\n",
    "    tokens = sent.split()\n",
    "    parser = parse.FeatureEarleyChartParser(gr)\n",
    "    trees = parser.parse(tokens)\n",
    "    for tree in trees: print(tree)\n",
    "sent1 = \"the farmer loaded the cart with sand\"\n",
    "sent2 = \"the farmer loaded sand into the cart\"\n",
    "sent3 = \"the farmer filled the cart with sand\"\n",
    "sent4 = \"the farmer filled sand into the cart\"\n",
    "sent5 = \"the farmer dumped the cart with sand\"\n",
    "sent6 = \"the farmer dumped sand into the cart\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Grammar does not cover some of the input words: \"'with'\".",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_19352/505865944.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mparse_sent\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msent1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mgr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_19352/3291499258.py\u001b[0m in \u001b[0;36mparse_sent\u001b[1;34m(sent, gr)\u001b[0m\n\u001b[0;32m     30\u001b[0m     \u001b[0mtokens\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparse\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mFeatureEarleyChartParser\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 32\u001b[1;33m     \u001b[0mtrees\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     33\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mtree\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtrees\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtree\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[0msent1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"the farmer loaded the cart with sand\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\nltk\\parse\\chart.py\u001b[0m in \u001b[0;36mparse\u001b[1;34m(self, tokens, tree_class)\u001b[0m\n\u001b[0;32m   1472\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1473\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mparse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtokens\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtree_class\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mTree\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1474\u001b[1;33m         \u001b[0mchart\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchart_parse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1475\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0miter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mchart\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparses\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_grammar\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtree_class\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtree_class\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1476\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\nltk\\parse\\earleychart.py\u001b[0m in \u001b[0;36mchart_parse\u001b[1;34m(self, tokens, trace)\u001b[0m\n\u001b[0;32m    350\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    351\u001b[0m         \u001b[0mtokens\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 352\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_grammar\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcheck_coverage\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    353\u001b[0m         \u001b[0mchart\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_chart_class\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    354\u001b[0m         \u001b[0mgrammar\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_grammar\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\nltk\\grammar.py\u001b[0m in \u001b[0;36mcheck_coverage\u001b[1;34m(self, tokens)\u001b[0m\n\u001b[0;32m    662\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mmissing\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    663\u001b[0m             \u001b[0mmissing\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\", \"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"{w!r}\"\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmissing\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 664\u001b[1;33m             raise ValueError(\n\u001b[0m\u001b[0;32m    665\u001b[0m                 \u001b[1;34m\"Grammar does not cover some of the \"\u001b[0m \u001b[1;34m\"input words: %r.\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mmissing\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    666\u001b[0m             )\n",
      "\u001b[1;31mValueError\u001b[0m: Grammar does not cover some of the input words: \"'with'\"."
     ]
    }
   ],
   "source": [
    "parse_sent(sent1,gr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 5\n",
    "\n",
    "Consider the following feature structures:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs1 = nltk.FeatStruct(\"[A = ?x, B= [C = ?x]]\")\n",
    "fs2 = nltk.FeatStruct(\"[B = [D = d]]\")\n",
    "fs3 = nltk.FeatStruct(\"[B = [C = d]]\")\n",
    "fs4 = nltk.FeatStruct(\"[A = (1)[B = b], C->(1)]\")\n",
    "fs5 = nltk.FeatStruct(\"[A = (1)[D = ?x], C = [E -> (1), F = ?x] ]\")\n",
    "fs6 = nltk.FeatStruct(\"[A = [D = d]]\")\n",
    "fs7 = nltk.FeatStruct(\"[A = [D = d], C = [F = [D = d]]]\")\n",
    "fs8 = nltk.FeatStruct(\"[A = (1)[D = ?x, G = ?x], C = [B = ?x, E -> (1)] ]\")\n",
    "fs9 = nltk.FeatStruct(\"[A = [B = b], C = [E = [G = e]]]\")\n",
    "fs10 = nltk.FeatStruct(\"[A = (1)[B = b], C -> (1)]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the result of the following unifications? \n",
    "\n",
    "1. `fs1` and `fs2`,  \n",
    "2. `fs1` and `fs3`,  \n",
    "3. `fs4` and `fs5`,  \n",
    "4. `fs5` and `fs6`,  \n",
    "5. `fs5` and `fs7`,  \n",
    "6. `fs8` and `fs9`,  \n",
    "7. `fs8` and `fs10`.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ A = ?x          ]\n",
      "[                 ]\n",
      "[ B = [ C = ?x  ] ]\n",
      "[     [ D = 'd' ] ]\n"
     ]
    }
   ],
   "source": [
    "print(fs1.unify(fs2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ A = 'd'         ]\n",
      "[                 ]\n",
      "[ B = [ C = 'd' ] ]\n"
     ]
    }
   ],
   "source": [
    "print(fs1.unify(fs3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[         [ B = 'b'  ] ]\n",
      "[ A = (1) [ D = ?x   ] ]\n",
      "[         [ E -> (1) ] ]\n",
      "[         [ F = ?x   ] ]\n",
      "[                      ]\n",
      "[ C -> (1)             ]\n"
     ]
    }
   ],
   "source": [
    "print(fs4.unify(fs5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ A = (1) [ D = 'd' ] ]\n",
      "[                     ]\n",
      "[ C = [ E -> (1) ]    ]\n",
      "[     [ F = 'd'  ]    ]\n"
     ]
    }
   ],
   "source": [
    "print(fs5.unify(fs6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(fs5.unify(fs7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[         [ B = 'b' ] ]\n",
      "[ A = (1) [ D = 'e' ] ]\n",
      "[         [ G = 'e' ] ]\n",
      "[                     ]\n",
      "[ C = [ B = 'e'  ]    ]\n",
      "[     [ E -> (1) ]    ]\n"
     ]
    }
   ],
   "source": [
    "print(fs8.unify(fs9))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[         [ B = 'b'  ] ]\n",
      "[ A = (1) [ D = 'b'  ] ]\n",
      "[         [ E -> (1) ] ]\n",
      "[         [ G = 'b'  ] ]\n",
      "[                      ]\n",
      "[ C -> (1)             ]\n"
     ]
    }
   ],
   "source": [
    "print(fs8.unify(fs10))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
