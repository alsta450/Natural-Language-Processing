{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# 6. Learning to Classify Text\n",
    "\n",
    "Detecting patterns is a central part of Natural Language Processing. Words ending in *-ed* tend to be past tense verbs. Frequent use of *will* is indicative of news text. These observable patterns — word structure and word frequency — happen to correlate with particular aspects of meaning, such as tense and topic. But how did we know where to start looking, which aspects of form to associate with which aspects of meaning?\n",
    "\n",
    "The goal of this chapter is to answer the following questions:\n",
    "\n",
    "1.    How can we identify particular features of language data that are salient for classifying it?\n",
    "2.    How can we construct models of language that can be used to perform language processing tasks automatically?\n",
    "3.    What can we learn about language from these models?\n",
    "\n",
    "Along the way we will study some important machine learning techniques, including decision trees, naive Bayes' classifiers, and maximum entropy classifiers. We will gloss over the mathematical and statistical underpinnings of these techniques, focusing instead on how and when to use them. Before looking at these methods, we first need to appreciate the broad scope of this topic."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1   Supervised Classification\n",
    "\n",
    "**Classification** is the task of choosing the correct **class label** for a given input. In basic classification tasks, each input is considered in isolation from all other inputs, and the set of labels is defined in advance. Some examples of classification tasks are:\n",
    "\n",
    "-    Deciding whether an email is spam or not.\n",
    "-    Deciding what the topic of a news article is, from a fixed list of topic areas such as \"sports,\" \"technology,\" and \"politics.\"\n",
    "-    Deciding whether a given occurrence of the word *bank* is used to refer to a river bank, a financial institution, the act of tilting to the side, or the act of depositing something in a financial institution.\n",
    "\n",
    "The basic classification task has a number of interesting variants. For example, in multi-class classification, each instance may be assigned multiple labels; in open-class classification, the set of labels is not defined in advance; and in sequence classification, a list of inputs are jointly classified.\n",
    "\n",
    "A classifier is called **supervised** if it is built based on training corpora containing the correct label for each input. The framework used by supervised classification is shown in the following figure:\n",
    "\n",
    "<img src='images/supervised-classification.png' width='400' />\n",
    "\n",
    "*Supervised Classification. (a) During training, a feature extractor is used to convert each input value to a feature set. These feature sets, which capture the basic information about each input that should be used to classify it, are discussed in the next section. Pairs of feature sets and labels are fed into the machine learning algorithm to generate a model. (b) During prediction, the same feature extractor is used to convert unseen inputs to feature sets. These feature sets are then fed into the model, which generates predicted labels.*\n",
    "\n",
    "In the rest of this section, we will look at how classifiers can be employed to solve a wide variety of tasks. Our discussion is not intended to be comprehensive, but to give a representative sample of tasks that can be performed with the help of text classifiers."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1.1   Gender Identification\n",
    "\n",
    "In Chapter 2 we saw that male and female names have some distinctive characteristics. Names ending in *a*, *e* and *i* are likely to be female, while names ending in *k*, *o*, *r*, *s* and *t* are likely to be male. Let's build a classifier to model these differences more precisely.\n",
    "\n",
    "The first step in creating a classifier is deciding what **features** of the input are relevant, and how to **encode** those features. For this example, we'll start by just looking at the final letter of a given name. The following **feature extractor** function builds a dictionary containing relevant information about a given name:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "def gender_features(word):\r\n",
    "    return {'last_letter': word[-1]}\r\n",
    "gender_features('Shrek')"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'last_letter': 'k'}"
      ]
     },
     "metadata": {},
     "execution_count": 1
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The returned dictionary, known as a **feature set**, maps from feature names to their values. Feature names are case-sensitive strings that typically provide a short human-readable description of the feature, as in the example `'last_letter'`. Feature values are values with simple types, such as booleans, numbers, and strings.\n",
    "\n",
    "Now that we've defined a feature extractor, we need to prepare a list of examples and corresponding class labels."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "from nltk.corpus import names\r\n",
    "labeled_names = ([(name, 'male') for name in names.words('male.txt')] +\r\n",
    "[(name, 'female') for name in names.words('female.txt')])\r\n",
    "import random\r\n",
    "random.shuffle(labeled_names)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Next, we use the feature extractor to process the `names` data, and divide the resulting list of feature sets into a **training set** and a **test set**. The training set is used to train a new \"naive Bayes\" classifier."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "import nltk\r\n",
    "featuresets = [(gender_features(n), gender) for (n, gender) in labeled_names]\r\n",
    "train_set, test_set = featuresets[500:], featuresets[:500]\r\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "We will learn more about the naive Bayes classifier later in the chapter. For now, let's just test it out on some names that did not appear in its training data:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "classifier.classify(gender_features('Neo'))"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'male'"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "classifier.classify(gender_features('Trinity'))"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'female'"
      ]
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Observe that these character names from *The Matrix* are correctly classified. Although this science fiction movie is set in 2199, it still conforms with our expectations about names and genders. We can systematically evaluate the classifier on a much larger quantity of unseen data:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "print(nltk.classify.accuracy(classifier, test_set))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0.738\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Finally, we can examine the classifier to determine which features it found most effective for distinguishing the names' genders:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "classifier.show_most_informative_features(5)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Most Informative Features\n",
      "             last_letter = 'k'              male : female =     46.5 : 1.0\n",
      "             last_letter = 'a'            female : male   =     40.0 : 1.0\n",
      "             last_letter = 'f'              male : female =     17.3 : 1.0\n",
      "             last_letter = 'p'              male : female =     12.6 : 1.0\n",
      "             last_letter = 'm'              male : female =     10.6 : 1.0\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "This listing shows that the names in the training set that end in \"a\" are female over 30 times more often than they are male, but names that end in \"k\" are male over 30 times more often than they are female. These ratios are known as **likelihood ratios**, and can be useful for comparing different feature-outcome relationships.\n",
    "\n",
    "When working with large corpora, constructing a single list that contains the features of every instance can use up a large amount of memory. In these cases, use the function `nltk.classify.apply_features`, which returns an object that acts like a list but does not store all the feature sets in memory:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "from nltk.classify import apply_features\r\n",
    "train_set = apply_features(gender_features, labeled_names[500:])\r\n",
    "test_set = apply_features(gender_features, labeled_names[:500])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1.2   Choosing The Right Features\n",
    "\n",
    "Selecting relevant features and deciding how to encode them for a learning method can have an enormous impact on the learning method's ability to extract a good model. Much of the interesting work in building a classifier is deciding what features might be relevant, and how we can represent them. Although it's often possible to get decent performance by using a fairly simple and obvious set of features, there are usually significant gains to be had by using carefully constructed features based on a thorough understanding of the task at hand.\n",
    "\n",
    "Typically, feature extractors are built through a process of trial-and-error, guided by intuitions about what information is relevant to the problem. It's common to start with a \"kitchen sink\" approach, including all the features that you can think of, and then checking to see which features actually are helpful. We take this approach for name gender features:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "def gender_features2(name):\r\n",
    "    features = {}\r\n",
    "    features[\"first_letter\"] = name[0].lower()\r\n",
    "    features[\"last_letter\"] = name[-1].lower()\r\n",
    "    for letter in 'abcdefghijklmnopqrstuvwxyz':\r\n",
    "        features[\"count({})\".format(letter)] = name.lower().count(letter)\r\n",
    "        features[\"has({})\".format(letter)] = (letter in name.lower())\r\n",
    "    return features\r\n",
    "\r\n",
    "gender_features2('John') "
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'first_letter': 'j',\n",
       " 'last_letter': 'n',\n",
       " 'count(a)': 0,\n",
       " 'has(a)': False,\n",
       " 'count(b)': 0,\n",
       " 'has(b)': False,\n",
       " 'count(c)': 0,\n",
       " 'has(c)': False,\n",
       " 'count(d)': 0,\n",
       " 'has(d)': False,\n",
       " 'count(e)': 0,\n",
       " 'has(e)': False,\n",
       " 'count(f)': 0,\n",
       " 'has(f)': False,\n",
       " 'count(g)': 0,\n",
       " 'has(g)': False,\n",
       " 'count(h)': 1,\n",
       " 'has(h)': True,\n",
       " 'count(i)': 0,\n",
       " 'has(i)': False,\n",
       " 'count(j)': 1,\n",
       " 'has(j)': True,\n",
       " 'count(k)': 0,\n",
       " 'has(k)': False,\n",
       " 'count(l)': 0,\n",
       " 'has(l)': False,\n",
       " 'count(m)': 0,\n",
       " 'has(m)': False,\n",
       " 'count(n)': 1,\n",
       " 'has(n)': True,\n",
       " 'count(o)': 1,\n",
       " 'has(o)': True,\n",
       " 'count(p)': 0,\n",
       " 'has(p)': False,\n",
       " 'count(q)': 0,\n",
       " 'has(q)': False,\n",
       " 'count(r)': 0,\n",
       " 'has(r)': False,\n",
       " 'count(s)': 0,\n",
       " 'has(s)': False,\n",
       " 'count(t)': 0,\n",
       " 'has(t)': False,\n",
       " 'count(u)': 0,\n",
       " 'has(u)': False,\n",
       " 'count(v)': 0,\n",
       " 'has(v)': False,\n",
       " 'count(w)': 0,\n",
       " 'has(w)': False,\n",
       " 'count(x)': 0,\n",
       " 'has(x)': False,\n",
       " 'count(y)': 0,\n",
       " 'has(y)': False,\n",
       " 'count(z)': 0,\n",
       " 'has(z)': False}"
      ]
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "However, there are usually limits to the number of features that you should use with a given learning algorithm — if you provide too many features, then the algorithm will have a higher chance of relying on idiosyncrasies of your training data that don't generalize well to new examples. This problem is known as **overfitting**, and can be especially problematic when working with small training sets. For example, if we train a naive Bayes classifier using the previous feature extractor, it will overfit the relatively small training set, resulting in a system whose accuracy is about 1% lower than the accuracy of a classifier that only pays attention to the final letter of each name:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "featuresets = [(gender_features2(n), gender) for (n, gender) in labeled_names]\r\n",
    "train_set, test_set = featuresets[500:], featuresets[:500]\r\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set)\r\n",
    "print(nltk.classify.accuracy(classifier, test_set))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0.754\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Once an initial set of features has been chosen, a very productive method for refining the feature set is **error analysis**. First, we select a **development set**, containing the corpus data for creating the model. This development set is then subdivided into the **training set** and the **dev-test set**."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "train_names = labeled_names[1500:]\r\n",
    "devtest_names = labeled_names[500:1500]\r\n",
    "test_names = labeled_names[:500]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The training set is used to train the model, and the dev-test set is used to perform error analysis. The test set serves in our final evaluation of the system. For reasons discussed below, it is important that we employ a separate dev-test set for error analysis, rather than just using the test set. The division of the corpus data into different subsets is shown in the following figure.\n",
    "\n",
    "<img src='images/corpus-org.png' width='400' />\n",
    "\n",
    "*Organization of corpus data for training supervised classifiers. The corpus data is divided into two sets: the development set, and the test set. The development set is often further subdivided into a training set and a dev-test set.*\n",
    "\n",
    "Having divided the corpus into appropriate datasets, we train a model using the training set, and then run it on the dev-test set."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "train_set = [(gender_features(n), gender) for (n, gender) in train_names]\r\n",
    "devtest_set = [(gender_features(n), gender) for (n, gender) in devtest_names]\r\n",
    "test_set = [(gender_features(n), gender) for (n, gender) in test_names]\r\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set)\r\n",
    "print(nltk.classify.accuracy(classifier, devtest_set))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0.764\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Using the dev-test set, we can generate a list of the errors that the classifier makes when predicting name genders:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "errors = []\r\n",
    "for (name, tag) in devtest_names:\r\n",
    "    guess = classifier.classify(gender_features(name))\r\n",
    "    if guess != tag:\r\n",
    "        errors.append( (tag, guess, name) )"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can then examine individual error cases where the model predicted the wrong label, and try to determine what additional pieces of information would allow it to make the right decision (or which existing pieces of information are tricking it into making the wrong decision). The feature set can then be adjusted accordingly. The names classifier that we have built generates about 100 errors on the dev-test corpus:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "for (tag, guess, name) in sorted(errors):\r\n",
    "    print('correct={:<8} guess={:<8s} name={:<30}'.format(tag, guess, name))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "correct=female   guess=male     name=Adriaens                      \n",
      "correct=female   guess=male     name=Aimil                         \n",
      "correct=female   guess=male     name=Allyson                       \n",
      "correct=female   guess=male     name=Anet                          \n",
      "correct=female   guess=male     name=Avis                          \n",
      "correct=female   guess=male     name=Barb                          \n",
      "correct=female   guess=male     name=Bess                          \n",
      "correct=female   guess=male     name=Bliss                         \n",
      "correct=female   guess=male     name=Bo                            \n",
      "correct=female   guess=male     name=Bren                          \n",
      "correct=female   guess=male     name=Britt                         \n",
      "correct=female   guess=male     name=Brittan                       \n",
      "correct=female   guess=male     name=Bryn                          \n",
      "correct=female   guess=male     name=Caril                         \n",
      "correct=female   guess=male     name=Carlynn                       \n",
      "correct=female   guess=male     name=Caron                         \n",
      "correct=female   guess=male     name=Carroll                       \n",
      "correct=female   guess=male     name=Cass                          \n",
      "correct=female   guess=male     name=Cat                           \n",
      "correct=female   guess=male     name=Cathleen                      \n",
      "correct=female   guess=male     name=Charlot                       \n",
      "correct=female   guess=male     name=Cher                          \n",
      "correct=female   guess=male     name=Cherilynn                     \n",
      "correct=female   guess=male     name=Chrystel                      \n",
      "correct=female   guess=male     name=Clovis                        \n",
      "correct=female   guess=male     name=Consuelo                      \n",
      "correct=female   guess=male     name=Cristabel                     \n",
      "correct=female   guess=male     name=Cristin                       \n",
      "correct=female   guess=male     name=Cyb                           \n",
      "correct=female   guess=male     name=Daryl                         \n",
      "correct=female   guess=male     name=Del                           \n",
      "correct=female   guess=male     name=Demeter                       \n",
      "correct=female   guess=male     name=Diamond                       \n",
      "correct=female   guess=male     name=Dido                          \n",
      "correct=female   guess=male     name=Dorian                        \n",
      "correct=female   guess=male     name=Doris                         \n",
      "correct=female   guess=male     name=Dorris                        \n",
      "correct=female   guess=male     name=Dot                           \n",
      "correct=female   guess=male     name=Eden                          \n",
      "correct=female   guess=male     name=Eileen                        \n",
      "correct=female   guess=male     name=Elisabet                      \n",
      "correct=female   guess=male     name=Ellen                         \n",
      "correct=female   guess=male     name=Em                            \n",
      "correct=female   guess=male     name=Enid                          \n",
      "correct=female   guess=male     name=Ethelyn                       \n",
      "correct=female   guess=male     name=Ethyl                         \n",
      "correct=female   guess=male     name=Fan                           \n",
      "correct=female   guess=male     name=Fleur                         \n",
      "correct=female   guess=male     name=Gennifer                      \n",
      "correct=female   guess=male     name=Gillan                        \n",
      "correct=female   guess=male     name=Glynis                        \n",
      "correct=female   guess=male     name=Gus                           \n",
      "correct=female   guess=male     name=Idell                         \n",
      "correct=female   guess=male     name=Ines                          \n",
      "correct=female   guess=male     name=Iris                          \n",
      "correct=female   guess=male     name=Isabeau                       \n",
      "correct=female   guess=male     name=Iseabal                       \n",
      "correct=female   guess=male     name=Isobel                        \n",
      "correct=female   guess=male     name=Jacquelin                     \n",
      "correct=female   guess=male     name=Jo-Ann                        \n",
      "correct=female   guess=male     name=Jocelin                       \n",
      "correct=female   guess=male     name=Joell                         \n",
      "correct=female   guess=male     name=Jonell                        \n",
      "correct=female   guess=male     name=Jordan                        \n",
      "correct=female   guess=male     name=Joslyn                        \n",
      "correct=female   guess=male     name=Juliann                       \n",
      "correct=female   guess=male     name=Kaitlyn                       \n",
      "correct=female   guess=male     name=Kameko                        \n",
      "correct=female   guess=male     name=Karen                         \n",
      "correct=female   guess=male     name=Karilynn                      \n",
      "correct=female   guess=male     name=Karleen                       \n",
      "correct=female   guess=male     name=Kat                           \n",
      "correct=female   guess=male     name=Katlin                        \n",
      "correct=female   guess=male     name=Kerrin                        \n",
      "correct=female   guess=male     name=Kip                           \n",
      "correct=female   guess=male     name=Kirstyn                       \n",
      "correct=female   guess=male     name=Koren                         \n",
      "correct=female   guess=male     name=Kristan                       \n",
      "correct=female   guess=male     name=Linnell                       \n",
      "correct=female   guess=male     name=Lois                          \n",
      "correct=female   guess=male     name=Loreen                        \n",
      "correct=female   guess=male     name=Lorilyn                       \n",
      "correct=female   guess=male     name=Lynnett                       \n",
      "correct=female   guess=male     name=Madelyn                       \n",
      "correct=female   guess=male     name=Maridel                       \n",
      "correct=female   guess=male     name=Mariel                        \n",
      "correct=female   guess=male     name=Meaghan                       \n",
      "correct=female   guess=male     name=Mellisent                     \n",
      "correct=female   guess=male     name=Meryl                         \n",
      "correct=female   guess=male     name=Michell                       \n",
      "correct=female   guess=male     name=Mikako                        \n",
      "correct=female   guess=male     name=Milicent                      \n",
      "correct=female   guess=male     name=Murial                        \n",
      "correct=female   guess=male     name=Nell                          \n",
      "correct=female   guess=male     name=Noel                          \n",
      "correct=female   guess=male     name=Noellyn                       \n",
      "correct=female   guess=male     name=Opal                          \n",
      "correct=female   guess=male     name=Pam                           \n",
      "correct=female   guess=male     name=Phyllys                       \n",
      "correct=female   guess=male     name=Quinn                         \n",
      "correct=female   guess=male     name=Raf                           \n",
      "correct=female   guess=male     name=Rahal                         \n",
      "correct=female   guess=male     name=Raven                         \n",
      "correct=female   guess=male     name=Robinet                       \n",
      "correct=female   guess=male     name=Rosaleen                      \n",
      "correct=female   guess=male     name=Ryann                         \n",
      "correct=female   guess=male     name=Sam                           \n",
      "correct=female   guess=male     name=Scarlett                      \n",
      "correct=female   guess=male     name=Shanon                        \n",
      "correct=female   guess=male     name=Sharleen                      \n",
      "correct=female   guess=male     name=Shel                          \n",
      "correct=female   guess=male     name=Sher                          \n",
      "correct=female   guess=male     name=Starr                         \n",
      "correct=female   guess=male     name=Stoddard                      \n",
      "correct=female   guess=male     name=Susann                        \n",
      "correct=female   guess=male     name=Veradis                       \n",
      "correct=female   guess=male     name=Vin                           \n",
      "correct=male     guess=female   name=Abbey                         \n",
      "correct=male     guess=female   name=Abe                           \n",
      "correct=male     guess=female   name=Ambrose                       \n",
      "correct=male     guess=female   name=Amory                         \n",
      "correct=male     guess=female   name=Anatole                       \n",
      "correct=male     guess=female   name=Andrea                        \n",
      "correct=male     guess=female   name=Ari                           \n",
      "correct=male     guess=female   name=Arvie                         \n",
      "correct=male     guess=female   name=Ashley                        \n",
      "correct=male     guess=female   name=Bailey                        \n",
      "correct=male     guess=female   name=Barclay                       \n",
      "correct=male     guess=female   name=Bealle                        \n",
      "correct=male     guess=female   name=Bela                          \n",
      "correct=male     guess=female   name=Berkley                       \n",
      "correct=male     guess=female   name=Blake                         \n",
      "correct=male     guess=female   name=Brice                         \n",
      "correct=male     guess=female   name=Buddy                         \n",
      "correct=male     guess=female   name=Burnaby                       \n",
      "correct=male     guess=female   name=Chase                         \n",
      "correct=male     guess=female   name=Chrissy                       \n",
      "correct=male     guess=female   name=Cobby                         \n",
      "correct=male     guess=female   name=Cody                          \n",
      "correct=male     guess=female   name=Constantine                   \n",
      "correct=male     guess=female   name=Corky                         \n",
      "correct=male     guess=female   name=Costa                         \n",
      "correct=male     guess=female   name=Dale                          \n",
      "correct=male     guess=female   name=Dmitri                        \n",
      "correct=male     guess=female   name=Drake                         \n",
      "correct=male     guess=female   name=Duffie                        \n",
      "correct=male     guess=female   name=Eli                           \n",
      "correct=male     guess=female   name=Emory                         \n",
      "correct=male     guess=female   name=Filmore                       \n",
      "correct=male     guess=female   name=Garth                         \n",
      "correct=male     guess=female   name=Gary                          \n",
      "correct=male     guess=female   name=Geoffry                       \n",
      "correct=male     guess=female   name=Gere                          \n",
      "correct=male     guess=female   name=Germaine                      \n",
      "correct=male     guess=female   name=Giovanni                      \n",
      "correct=male     guess=female   name=Haley                         \n",
      "correct=male     guess=female   name=Helmuth                       \n",
      "correct=male     guess=female   name=Henrique                      \n",
      "correct=male     guess=female   name=Hersh                         \n",
      "correct=male     guess=female   name=Hugh                          \n",
      "correct=male     guess=female   name=Ira                           \n",
      "correct=male     guess=female   name=Isa                           \n",
      "correct=male     guess=female   name=Isaiah                        \n",
      "correct=male     guess=female   name=Isidore                       \n",
      "correct=male     guess=female   name=Jean-Christophe               \n",
      "correct=male     guess=female   name=Jeffie                        \n",
      "correct=male     guess=female   name=Jerry                         \n",
      "correct=male     guess=female   name=Jessie                        \n",
      "correct=male     guess=female   name=Jimmie                        \n",
      "correct=male     guess=female   name=Johnny                        \n",
      "correct=male     guess=female   name=Johny                         \n",
      "correct=male     guess=female   name=Jordy                         \n",
      "correct=male     guess=female   name=Keith                         \n",
      "correct=male     guess=female   name=Krishna                       \n",
      "correct=male     guess=female   name=Leslie                        \n",
      "correct=male     guess=female   name=Levi                          \n",
      "correct=male     guess=female   name=Levy                          \n",
      "correct=male     guess=female   name=Lex                           \n",
      "correct=male     guess=female   name=Lindsey                       \n",
      "correct=male     guess=female   name=Locke                         \n",
      "correct=male     guess=female   name=Lyle                          \n",
      "correct=male     guess=female   name=Maddie                        \n",
      "correct=male     guess=female   name=Marsh                         \n",
      "correct=male     guess=female   name=Matty                         \n",
      "correct=male     guess=female   name=Max                           \n",
      "correct=male     guess=female   name=Merry                         \n",
      "correct=male     guess=female   name=Micah                         \n",
      "correct=male     guess=female   name=Mike                          \n",
      "correct=male     guess=female   name=Mikey                         \n",
      "correct=male     guess=female   name=Mischa                        \n",
      "correct=male     guess=female   name=Mitch                         \n",
      "correct=male     guess=female   name=Moe                           \n",
      "correct=male     guess=female   name=Montgomery                    \n",
      "correct=male     guess=female   name=Mugsy                         \n",
      "correct=male     guess=female   name=Neddie                        \n",
      "correct=male     guess=female   name=Nicky                         \n",
      "correct=male     guess=female   name=Nikita                        \n",
      "correct=male     guess=female   name=Noah                          \n",
      "correct=male     guess=female   name=Obie                          \n",
      "correct=male     guess=female   name=Ossie                         \n",
      "correct=male     guess=female   name=Pace                          \n",
      "correct=male     guess=female   name=Patrice                       \n",
      "correct=male     guess=female   name=Prentice                      \n",
      "correct=male     guess=female   name=Pryce                         \n",
      "correct=male     guess=female   name=Rabi                          \n",
      "correct=male     guess=female   name=Reube                         \n",
      "correct=male     guess=female   name=Rex                           \n",
      "correct=male     guess=female   name=Richy                         \n",
      "correct=male     guess=female   name=Rocky                         \n",
      "correct=male     guess=female   name=Roscoe                        \n",
      "correct=male     guess=female   name=Rourke                        \n",
      "correct=male     guess=female   name=Rutledge                      \n",
      "correct=male     guess=female   name=Sarge                         \n",
      "correct=male     guess=female   name=Serge                         \n",
      "correct=male     guess=female   name=Shelby                        \n",
      "correct=male     guess=female   name=Sherlocke                     \n",
      "correct=male     guess=female   name=Shorty                        \n",
      "correct=male     guess=female   name=Sidney                        \n",
      "correct=male     guess=female   name=Skippie                       \n",
      "correct=male     guess=female   name=Slade                         \n",
      "correct=male     guess=female   name=Stearne                       \n",
      "correct=male     guess=female   name=Tabbie                        \n",
      "correct=male     guess=female   name=Tannie                        \n",
      "correct=male     guess=female   name=Teddy                         \n",
      "correct=male     guess=female   name=Timothy                       \n",
      "correct=male     guess=female   name=Tome                          \n",
      "correct=male     guess=female   name=Tonnie                        \n",
      "correct=male     guess=female   name=Verge                         \n",
      "correct=male     guess=female   name=Wallie                        \n",
      "correct=male     guess=female   name=Willey                        \n",
      "correct=male     guess=female   name=Willy                         \n",
      "correct=male     guess=female   name=Worth                         \n",
      "correct=male     guess=female   name=Yancey                        \n",
      "correct=male     guess=female   name=Zane                          \n",
      "correct=male     guess=female   name=Zechariah                     \n",
      "correct=male     guess=female   name=Zolly                         \n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Looking through this list of errors makes it clear that some suffixes that are more than one letter can be indicative of name genders. For example, names ending in *yn* appear to be predominantly female, despite the fact that names ending in *n* tend to be male; and names ending in *ch* are usually male, even though names that end in *h* tend to be female. We therefore adjust our feature extractor to include features for two-letter suffixes:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "def gender_features(word):\r\n",
    "    return {'suffix1': word[-1:],\r\n",
    "            'suffix2': word[-2:]}"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Rebuilding the classifier with the new feature extractor, we see that the performance on the dev-test dataset improves by almost 2 percentage points:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "train_set = [(gender_features(n), gender) for (n, gender) in train_names]\r\n",
    "devtest_set = [(gender_features(n), gender) for (n, gender) in devtest_names]\r\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set)\r\n",
    "print(nltk.classify.accuracy(classifier, devtest_set))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0.772\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "This error analysis procedure can then be repeated, checking for patterns in the errors that are made by the newly improved classifier. Each time the error analysis procedure is repeated, we should select a different dev-test/training split, to ensure that the classifier does not start to reflect idiosyncrasies in the dev-test set.\n",
    "\n",
    "But once we've used the dev-test set to help us develop the model, we can no longer trust that it will give us an accurate idea of how well the model would perform on new data. It is therefore important to keep the test set separate, and unused, until our model development is complete. At that point, we can use the test set to evaluate how well our model will perform on new input values."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1.3   Document Classification\r\n",
    "\r\n",
    "In Chapter 2, we saw several examples of corpora where documents have been labeled with categories. Using these corpora, we can build classifiers that will automatically tag new documents with appropriate category labels. First, we construct a list of documents, labeled with the appropriate categories. For this example, we've chosen the Movie Reviews Corpus, which categorizes each review as positive or negative."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from nltk.corpus import movie_reviews\r\n",
    "documents = [(list(movie_reviews.words(fileid)), category)\r\n",
    "             for category in movie_reviews.categories()\r\n",
    "             for fileid in movie_reviews.fileids(category)]\r\n",
    "random.shuffle(documents)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Next, we define a feature extractor for documents, so the classifier will know which aspects of the data it should pay attention to. For document topic identification, we can define a feature for each word, indicating whether the document contains that word. To limit the number of features that the classifier needs to process, we begin by constructing a list of the 2000 most frequent words in the overall corpus. We can then define a feature extractor that simply checks whether each of these words is present in a given document."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "all_words = nltk.FreqDist(w.lower() for w in movie_reviews.words())\r\n",
    "wf = all_words.most_common(2000)\r\n",
    "word_features = [w for (w,a) in wf]\r\n",
    "\r\n",
    "def document_features(document): \r\n",
    "    document_words = set(document)\r\n",
    "    features = {}\r\n",
    "    for word in word_features:\r\n",
    "        features['contains({})'.format(word)] = (word in document_words)\r\n",
    "    return features\r\n",
    "\r\n",
    "print(document_features(movie_reviews.words('pos/cv957_8737.txt'))) "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now that we've defined our feature extractor, we can use it to train a classifier to label new movie reviews. To check how reliable the resulting classifier is, we compute its accuracy on the test set. And once again, we can use `show_most_informative_features()` to find out which features the classifier found to be most informative."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "featuresets = [(document_features(d), c) for (d,c) in documents]\r\n",
    "train_set, test_set = featuresets[100:], featuresets[:100]\r\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set)\r\n",
    "\r\n",
    "print(nltk.classify.accuracy(classifier, test_set)) "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "classifier.show_most_informative_features(5) "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Apparently in this corpus, a review that mentions \"Seagal\" is almost 8 times more likely to be negative than positive, while a review that mentions \"Damon\" is about 6 times more likely to be positive."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1.4   Part-of-Speech Tagging\n",
    "\n",
    "In Chapter 5 we built a regular expression tagger that chooses a part-of-speech tag for a word by looking at the internal make-up of the word. However, this regular expression tagger had to be hand-crafted. Instead, we can train a classifier to work out which suffixes are most informative. Let's begin by finding out what the most common suffixes are:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from nltk.corpus import brown\r\n",
    "suffix_fdist = nltk.FreqDist()\r\n",
    "for word in brown.words():\r\n",
    "    word = word.lower()\r\n",
    "    suffix_fdist[word[-1:]] += 1\r\n",
    "    suffix_fdist[word[-2:]] += 1\r\n",
    "    suffix_fdist[word[-3:]] += 1\r\n",
    "\r\n",
    "common_suffixes = [suffix for (suffix, count) in suffix_fdist.most_common(100)]\r\n",
    "print(common_suffixes)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Next, we'll define a feature extractor function which checks a given word for these suffixes:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def pos_features(word):\r\n",
    "    features = {}\r\n",
    "    for suffix in common_suffixes:\r\n",
    "        features['endswith({})'.format(suffix)] = word.lower().endswith(suffix)\r\n",
    "    return features"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Feature extraction functions behave like tinted glasses, highlighting some of the properties (colors) in our data and making it impossible to see other properties. The classifier will rely exclusively on these highlighted properties when determining how to label inputs. In this case, the classifier will make its decisions based only on information about which of the common suffixes (if any) a given word has.\n",
    "\n",
    "Now that we've defined our feature extractor, we can use it to train a new \"decision tree\" classifier (to be discussed in Section 4). Attention: this may take a long time, depending on your hardware about 20 minutes."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "tagged_words = brown.tagged_words(categories='news')\r\n",
    "featuresets = [(pos_features(n), g) for (n,g) in tagged_words]\r\n",
    "size = int(len(featuresets) * 0.1)\r\n",
    "train_set, test_set = featuresets[size:], featuresets[:size]\r\n",
    "#classifier = nltk.DecisionTreeClassifier.train(train_set)\r\n",
    "#nltk.classify.accuracy(classifier, test_set)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#classifier.classify(pos_features('cats'))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "One nice feature of decision tree models is that they are often fairly easy to interpret — we can even instruct NLTK to print them out as pseudocode:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#print(classifier.pseudocode(depth=4))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1.5   Exploiting Context\n",
    "\n",
    "By augmenting the feature extraction function, we could modify this part-of-speech tagger to leverage a variety of other word-internal features, such as the length of the word, the number of syllables it contains, or its prefix. However, as long as the feature extractor just looks at the target word, we have no way to add features that depend on the *context* that the word appears in. But contextual features often provide powerful clues about the correct tag — for example, when tagging the word \"fly,\" knowing that the previous word is \"a\" will allow us to determine that it is functioning as a noun, not a verb.\n",
    "\n",
    "In order to accommodate features that depend on a word's context, we must revise the pattern that we used to define our feature extractor. Instead of just passing in the word to be tagged, we will pass in a complete (untagged) sentence, along with the index of the target word. This approach is demonstrated in the following listing, which employs a context-dependent feature extractor to define a part of speech tag classifier."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def pos_features(sentence, i): \r\n",
    "    features = {\"suffix(1)\": sentence[i][-1:],\r\n",
    "                \"suffix(2)\": sentence[i][-2:],\r\n",
    "                \"suffix(3)\": sentence[i][-3:]}\r\n",
    "    if i == 0:\r\n",
    "        features[\"prev-word\"] = \"<START>\"\r\n",
    "    else:\r\n",
    "        features[\"prev-word\"] = sentence[i-1]\r\n",
    "    return features\r\n",
    "\r\n",
    "pos_features(brown.sents()[0], 8)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "tagged_sents = brown.tagged_sents(categories='news')\r\n",
    "featuresets = []\r\n",
    "for tagged_sent in tagged_sents:\r\n",
    "    untagged_sent = nltk.tag.untag(tagged_sent)\r\n",
    "    for i, (word, tag) in enumerate(tagged_sent):\r\n",
    "        featuresets.append( (pos_features(untagged_sent, i), tag) )\r\n",
    "\r\n",
    "size = int(len(featuresets) * 0.1)\r\n",
    "train_set, test_set = featuresets[size:], featuresets[:size]\r\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set)\r\n",
    "\r\n",
    "nltk.classify.accuracy(classifier, test_set)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "It is clear that exploiting contextual features improves the performance of our part-of-speech tagger. For example, the classifier learns that a word is likely to be a noun if it comes immediately after the word \"large\" or the word \"gubernatorial\". However, it is unable to learn the generalization that a word is probably a noun if it follows an adjective, because it doesn't have access to the previous word's part-of-speech tag. In general, simple classifiers always treat each input as independent from all other inputs. In many contexts, this makes perfect sense. For example, decisions about whether names tend to be male or female can be made on a case-by-case basis. However, there are often cases, such as part-of-speech tagging, where we are interested in solving classification problems that are closely related to one another."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1.6   Sequence Classification\n",
    "\n",
    "In order to capture the dependencies between related classification tasks, we can use **joint classifier** models, which choose an appropriate labeling for a collection of related inputs. In the case of part-of-speech tagging, a variety of different **sequence classifier** models can be used to jointly choose part-of-speech tags for all the words in a given sentence.\n",
    "\n",
    "One sequence classification strategy, known as **consecutive classification** or **greedy sequence classification**, is to find the most likely class label for the first input, then to use that answer to help find the best label for the next input. The process can then be repeated until all of the inputs have been labeled. This is the approach that was taken by the bigram tagger from Chapter 5, which began by choosing a part-of-speech tag for the first word in the sentence, and then chose the tag for each subsequent word based on the word itself and the predicted tag for the previous word.\n",
    "\n",
    "This strategy is demonstrated below. First, we must augment our feature extractor function to take a `history` argument, which provides a list of the tags that we've predicted for the sentence so far. Each tag in `history` corresponds with a word in `sentence`. But note that `history` will only contain tags for words we've already classified, that is, words to the left of the target word. Thus, while it is possible to look at some features of words to the right of the target word, it is not possible to look at the tags for those words (since we haven't generated them yet).\n",
    "\n",
    "Having defined a feature extractor, we can proceed to build our sequence classifier. During training, we use the annotated tags to provide the appropriate history to the feature extractor, but when tagging new sentences, we generate the history list based on the output of the tagger itself."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def pos_features(sentence, i, history): \r\n",
    "     features = {\"suffix(1)\": sentence[i][-1:],\r\n",
    "                 \"suffix(2)\": sentence[i][-2:],\r\n",
    "                 \"suffix(3)\": sentence[i][-3:]}\r\n",
    "     if i == 0:\r\n",
    "         features[\"prev-word\"] = \"<START>\"\r\n",
    "         features[\"prev-tag\"] = \"<START>\"\r\n",
    "     else:\r\n",
    "         features[\"prev-word\"] = sentence[i-1]\r\n",
    "         features[\"prev-tag\"] = history[i-1]\r\n",
    "     return features\r\n",
    "\r\n",
    "class ConsecutivePosTagger(nltk.TaggerI): \r\n",
    "\r\n",
    "    def __init__(self, train_sents):\r\n",
    "        train_set = []\r\n",
    "        for tagged_sent in train_sents:\r\n",
    "            untagged_sent = nltk.tag.untag(tagged_sent)\r\n",
    "            history = []\r\n",
    "            for i, (word, tag) in enumerate(tagged_sent):\r\n",
    "                featureset = pos_features(untagged_sent, i, history)\r\n",
    "                train_set.append( (featureset, tag) )\r\n",
    "                history.append(tag)\r\n",
    "        self.classifier = nltk.NaiveBayesClassifier.train(train_set)\r\n",
    "\r\n",
    "    def tag(self, sentence):\r\n",
    "        history = []\r\n",
    "        for i, word in enumerate(sentence):\r\n",
    "            featureset = pos_features(sentence, i, history)\r\n",
    "            tag = self.classifier.classify(featureset)\r\n",
    "            history.append(tag)\r\n",
    "        return zip(sentence, history)\r\n",
    "\r\n",
    "tagged_sents = brown.tagged_sents(categories='news')\r\n",
    "size = int(len(tagged_sents) * 0.1)\r\n",
    "train_sents, test_sents = tagged_sents[size:], tagged_sents[:size]\r\n",
    "tagger = ConsecutivePosTagger(train_sents)\r\n",
    "print(tagger.evaluate(test_sents))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1.7   Other Methods for Sequence Classification\n",
    "\n",
    "One shortcoming of this approach is that we commit to every decision that we make. For example, if we decide to label a word as a noun, but later find evidence that it should have been a verb, there's no way to go back and fix our mistake. One solution to this problem is to adopt a transformational strategy instead. Transformational joint classifiers work by creating an initial assignment of labels for the inputs, and then iteratively refining that assignment in an attempt to repair inconsistencies between related inputs. The Brill tagger, described in Chapter 5, is a good example of this strategy.\n",
    "\n",
    "Another solution is to assign scores to all of the possible sequences of part-of-speech tags, and to choose the sequence whose overall score is highest. This is the approach taken by **Hidden Markov Models**. Hidden Markov Models are similar to consecutive classifiers in that they look at both the inputs and the history of predicted tags. However, rather than simply finding the single best tag for a given word, they generate a probability distribution over tags. These probabilities are then combined to calculate probability scores for tag sequences, and the tag sequence with the highest probability is chosen. Unfortunately, the number of possible tag sequences is quite large. Given a tag set with 30 tags, there are about 600 trillion ($30^{10}$) ways to label a 10-word sentence. In order to avoid considering all these possible sequences separately, Hidden Markov Models require that the feature extractor only look at the most recent tag (or the most recent $n$ tags, where $n$ is fairly small). Given that restriction, it is possible to use dynamic programming (see Chapter 4) to efficiently find the most likely tag sequence. In particular, for each consecutive word index $i$, a score is computed for each possible current and previous tag. This same basic approach is taken by two more advanced models, called **Maximum Entropy Markov Models** and **Linear-Chain Conditional Random Field Models**; but different algorithms are used to find scores for tag sequences."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2   Further Examples of Supervised Classification\n",
    "\n",
    "### 2.1   Sentence Segmentation\n",
    "\n",
    "Sentence segmentation can be viewed as a classification task for punctuation: whenever we encounter a symbol that could possibly end a sentence, such as a period or a question mark, we have to decide whether it terminates the preceding sentence.\n",
    "\n",
    "The first step is to obtain some data that has already been segmented into sentences and convert it into a form that is suitable for extracting features:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import nltk\r\n",
    "sents = nltk.corpus.treebank_raw.sents()\r\n",
    "tokens = []\r\n",
    "boundaries = set()\r\n",
    "offset = 0\r\n",
    "for sent in sents:\r\n",
    "    tokens.extend(sent)\r\n",
    "    offset += len(sent)\r\n",
    "    boundaries.add(offset-1)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Here, `tokens` is a merged list of tokens from the individual sentences, and `boundaries` is a set containing the indexes of all sentence-boundary tokens. Next, we need to specify the features of the data that will be used in order to decide whether punctuation indicates a sentence-boundary:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def punct_features(tokens, i):\r\n",
    "    return {'next-word-capitalized': tokens[i+1][0].isupper(),\r\n",
    "            'prev-word': tokens[i-1].lower(),\r\n",
    "            'punct': tokens[i],\r\n",
    "            'prev-word-is-one-char': len(tokens[i-1]) == 1}"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Based on this feature extractor, we can create a list of labeled featuresets by selecting all the punctuation tokens, and tagging whether they are boundary tokens or not:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "featuresets = [(punct_features(tokens, i), (i in boundaries))\r\n",
    "               for i in range(1, len(tokens)-1)\r\n",
    "               if tokens[i] in '.?!']"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Using these featuresets, we can train and evaluate a punctuation classifier:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "size = int(len(featuresets) * 0.1)\r\n",
    "train_set, test_set = featuresets[size:], featuresets[:size]\r\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set)\r\n",
    "nltk.classify.accuracy(classifier, test_set)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "To use this classifier to perform sentence segmentation, we simply check each punctuation mark to see whether it's labeled as a boundary; and divide the list of words at the boundary marks. The following listing shows how this can be done."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def segment_sentences(words):\r\n",
    "    start = 0\r\n",
    "    sents = []\r\n",
    "    for i, word in enumerate(words):\r\n",
    "        if word in '.?!' and classifier.classify(punct_features(words, i)) == True:\r\n",
    "            sents.append(words[start:i+1])\r\n",
    "            start = i+1\r\n",
    "    if start < len(words):\r\n",
    "        sents.append(words[start:])\r\n",
    "    return sents"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2.2   Identifying Dialogue Act Types\n",
    "\n",
    "When processing dialogue, it can be useful to think of utterances as a type of *action* performed by the speaker. This interpretation is most straightforward for performative statements such as \"I forgive you\" or \"I bet you can't climb that hill.\" But greetings, questions, answers, assertions, and clarifications can all be thought of as types of speech-based actions. Recognizing the **dialogue acts** underlying the utterances in a dialogue can be an important first step in understanding the conversation.\n",
    "\n",
    "The NPS Chat Corpus, which was demonstrated in Chapter 2, consists of over 10,000 posts from instant messaging sessions. These posts have all been labeled with one of 15 dialogue act types, such as \"Statement,\" \"Emotion,\" \"ynQuestion\", and \"Continuer.\" We can therefore use this data to build a classifier that can identify the dialogue act types for new instant messaging posts. The first step is to extract the basic messaging data. We will call `xml_posts()` to get a data structure representing the XML annotation for each post:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "posts = nltk.corpus.nps_chat.xml_posts()[:10000]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Next, we'll define a simple feature extractor that checks what words the post contains:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def dialogue_act_features(post):\r\n",
    "    features = {}\r\n",
    "    for word in nltk.word_tokenize(post):\r\n",
    "        features['contains({})'.format(word.lower())] = True\r\n",
    "    return features"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Finally, we construct the training and testing data by applying the feature extractor to each post (using `post.get('class')` to get a post's dialogue act type), and create a new classifier:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "featuresets = [(dialogue_act_features(post.text), post.get('class'))\r\n",
    "               for post in posts]\r\n",
    "size = int(len(featuresets) * 0.1)\r\n",
    "train_set, test_set = featuresets[size:], featuresets[:size]\r\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set)\r\n",
    "print(nltk.classify.accuracy(classifier, test_set))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2.3   Recognizing Textual Entailment\n",
    "\n",
    "Recognizing textual entailment (RTE) is the task of determining whether a given piece of text *T* entails another text *H* called the \"hypothesis\" (as already discussed in Chapter 1). To date, there have been four RTE Challenges, where shared development and test data is made available to competing teams. Here are a couple of examples of text/hypothesis pairs from the Challenge 3 development dataset. The label *True* indicates that the entailment holds, and *False*, that it fails to hold.\n",
    "\n",
    "*Challenge 3, Pair 34 (True)*\n",
    "\n",
    "**T**: Parviz Davudi was representing Iran at a meeting of the Shanghai Co-operation Organisation (SCO), the fledgling association that binds Russia, China and four former Soviet republics of central Asia together to fight terrorism.\n",
    "\n",
    "**H**: China is a member of SCO.\n",
    "\n",
    "*Challenge 3, Pair 81 (False)*\n",
    "\n",
    "**T**: According to NC Articles of Organization, the members of LLC company are H. Nelson Beavers, III, H. Chester Beavers and Jennie Beavers Stewart.\n",
    "\n",
    "**H**: Jennie Beavers Stewart is a share-holder of Carolina Analytical Laboratory.\n",
    "\n",
    "It should be emphasized that the relationship between text and hypothesis is not intended to be logical entailment, but rather whether a human would conclude that the text provides reasonable evidence for taking the hypothesis to be true.\n",
    "\n",
    "We can treat RTE as a classification task, in which we try to predict the *True/False* label for each pair. Although it seems likely that successful approaches to this task will involve a combination of parsing, semantics and real world knowledge, many early attempts at RTE achieved reasonably good results with shallow analysis, based on similarity between the text and hypothesis at the word level. In the ideal case, we would expect that if there is an entailment, then all the information expressed by the hypothesis should also be present in the text. Conversely, if there is information found in the hypothesis that is absent from the text, then there will be no entailment.\n",
    "\n",
    "In our RTE feature detector, we let words (i.e., word types) serve as proxies for information, and our features count the degree of word overlap, and the degree to which there are words in the hypothesis but not in the text (captured by the method `hyp_extra()`). Not all words are equally important — Named Entity mentions such as the names of people, organizations and places are likely to be more significant, which motivates us to extract distinct information for `word`s and `ne`s (Named Entities). In addition, some high frequency function words are filtered out as \"stopwords\"."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def rte_features(rtepair):\r\n",
    "    extractor = nltk.RTEFeatureExtractor(rtepair)\r\n",
    "    features = {}\r\n",
    "    features['word_overlap'] = len(extractor.overlap('word'))\r\n",
    "    features['word_hyp_extra'] = len(extractor.hyp_extra('word'))\r\n",
    "    features['ne_overlap'] = len(extractor.overlap('ne'))\r\n",
    "    features['ne_hyp_extra'] = len(extractor.hyp_extra('ne'))\r\n",
    "    return features"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "To illustrate the content of these features, we examine some attributes of the text/hypothesis Pair 34:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "rtepair = nltk.corpus.rte.pairs(['rte3_dev.xml'])[33]\r\n",
    "extractor = nltk.RTEFeatureExtractor(rtepair)\r\n",
    "print(extractor.text_words)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print(extractor.hyp_words)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print(extractor.overlap('word'))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print(extractor.overlap('ne'))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print(extractor.hyp_extra('word'))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "These features indicate that there is some evidence for labeling this as *True*.\n",
    "\n",
    "The module `nltk.classify.rte_classify` reaches just over 58% accuracy on the combined RTE test data using methods like these. Although this figure is not very impressive, it requires significant effort, and more linguistic processing, to achieve much better results."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2.4   Scaling Up to Large Datasets\n",
    "\n",
    "Python provides an excellent environment for performing basic text processing and feature extraction. However, it is not able to perform the numerically intensive calculations required by machine learning methods nearly as quickly as lower-level languages such as C. Thus, if you attempt to use the pure-Python machine learning implementations (such as `nltk.NaiveBayesClassifier`) on large datasets, you may find that the learning algorithm takes an unreasonable amount of time and memory to complete.\n",
    "\n",
    "If you plan to train classifiers with large amounts of training data or a large number of features, we recommend that you explore NLTK's facilities for interfacing with external machine learning packages. Once these packages have been installed, NLTK can transparently invoke them (via system calls) to train classifier models significantly faster than the pure-Python classifier implementations. See the NLTK webpage for a list of recommended machine learning packages that are supported by NLTK."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3   Evaluation\n",
    "\n",
    "In order to decide whether a classification model is accurately capturing a pattern, we must evaluate that model. The result of this evaluation is important for deciding how trustworthy the model is, and for what purposes we can use it. Evaluation can also be an effective tool for guiding us in making future improvements to the model.\n",
    "\n",
    "### 3.1   The Test Set\n",
    "\n",
    "Most evaluation techniques calculate a score for a model by comparing the labels that it generates for the inputs in a **test set** (or **evaluation set**) with the correct labels for those inputs. This test set typically has the same format as the training set. However, it is very important that the test set be distinct from the training corpus: if we simply re-used the training set as the test set, then a model that simply memorized its input, without learning how to generalize to new examples, would receive misleadingly high scores.\n",
    "\n",
    "When building the test set, there is often a trade-off between the amount of data available for testing and the amount available for training. For classification tasks that have a small number of well-balanced labels and a diverse test set, a meaningful evaluation can be performed with as few as 100 evaluation instances. But if a classification task has a large number of labels, or includes very infrequent labels, then the size of the test set should be chosen to ensure that the least frequent label occurs at least 50 times. Additionally, if the test set contains many closely related instances — such as instances drawn from a single document — then the size of the test set should be increased to ensure that this lack of diversity does not skew the evaluation results. When large amounts of annotated data are available, it is common to err on the side of safety by using 10% of the overall data for evaluation.\n",
    "\n",
    "Another consideration when choosing the test set is the degree of similarity between instances in the test set and those in the development set. The more similar these two datasets are, the less confident we can be that evaluation results will generalize to other datasets. For example, consider the part-of-speech tagging task. At one extreme, we could create the training set and test set by randomly assigning sentences from a data source that reflects a single genre (news):"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import random\r\n",
    "from nltk.corpus import brown\r\n",
    "tagged_sents = list(brown.tagged_sents(categories='news'))\r\n",
    "random.shuffle(tagged_sents)\r\n",
    "size = int(len(tagged_sents) * 0.1)\r\n",
    "train_set, test_set = tagged_sents[size:], tagged_sents[:size]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "In this case, our test set will be very similar to our training set. The training set and test set are taken from the same genre, and so we cannot be confident that evaluation results would generalize to other genres. What's worse, because of the call to `random.shuffle()`, the test set contains sentences that are taken from the same documents that were used for training. If there is any consistent pattern within a document — say, if a given word appears with a particular part-of-speech tag especially frequently — then that difference will be reflected in both the development set and the test set. A somewhat better approach is to ensure that the training set and test set are taken from different documents:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "file_ids = brown.fileids(categories='news')\r\n",
    "size = int(len(file_ids) * 0.1)\r\n",
    "train_set = brown.tagged_sents(file_ids[size:])\r\n",
    "test_set = brown.tagged_sents(file_ids[:size])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "If we want to perform a more stringent evaluation, we can draw the test set from documents that are less closely related to those in the training set:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "train_set = brown.tagged_sents(categories='news')\r\n",
    "test_set = brown.tagged_sents(categories='fiction')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "If we build a classifier that performs well on this test set, then we can be confident that it has the power to generalize well beyond the data that it was trained on."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.2   Accuracy\n",
    "\n",
    "The simplest metric that can be used to evaluate a classifier, **accuracy**, measures the percentage of inputs in the test set that the classifier correctly labeled. For example, a name gender classifier that predicts the correct name 60 times in a test set containing 80 names would have an accuracy of 60/80 = 75%. The function `nltk.classify.accuracy()` will calculate the accuracy of a classifier model on a given test set:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "    classifier = nltk.NaiveBayesClassifier.train(train_set) \n",
    "    print('Accuracy: {:4.2f}'.format(nltk.classify.accuracy(classifier, test_set)))"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "When interpreting the accuracy score of a classifier, it is important to take into consideration the frequencies of the individual class labels in the test set. For example, consider a classifier that determines the correct word sense for each occurrence of the word *bank*. If we evaluate this classifier on financial newswire text, then we may find that the `financial-institution` sense appears 19 times out of 20. In that case, an accuracy of 95% would hardly be impressive, since we could achieve that accuracy with a model that always returns the `financial-institution` sense. However, if we instead evaluate the classifier on a more balanced corpus, where the most frequent word sense has a frequency of 40%, then a 95% accuracy score would be a much more positive result. (A similar issue arises when measuring inter-annotator agreement.)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.3   Precision and Recall\n",
    "\n",
    "Another instance where accuracy scores can be misleading is in \"search\" tasks, such as information retrieval, where we are attempting to find documents that are relevant to a particular task. Since the number of irrelevant documents far outweighs the number of relevant documents, the accuracy score for a model that labels every document as irrelevant would be very close to 100%.\n",
    "\n",
    "<img src='images/precision-recall.png' width='400' />\n",
    "\n",
    "*True and False Positives and Negatives*\n",
    "\n",
    "It is therefore conventional to employ a different set of measures for search tasks, based on the number of items in each of the four categories shown above:\n",
    "\n",
    "-    **True positives** are relevant items that we correctly identified as relevant.\n",
    "-    **True negatives** are irrelevant items that we correctly identified as irrelevant.\n",
    "-    **False positives** (or **Type I errors**) are irrelevant items that we incorrectly identified as relevant.\n",
    "-    **False negatives** (or **Type II errors**) are relevant items that we incorrectly identified as irrelevant.\n",
    "\n",
    "Given these four numbers, we can define the following metrics:\n",
    "\n",
    "-    **Precision**, which indicates how many of the items that we identified were relevant, is *TP*/(*TP*+*FP*).\n",
    "-    **Recall**, which indicates how many of the relevant items we identified, is *TP*/(*TP*+*FN*).\n",
    "-    The **F-Measure** (or **F-Score**), which combines the precision and recall to give a single score, is defined to be the harmonic mean of the precision and recall: (2 × *Precision* × *Recall*) / (*Precision* + *Recall*)."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.4   Confusion Matrices\n",
    "\n",
    "When performing classification tasks with three or more labels, it can be informative to subdivide the errors made by the model based on which types of mistake it made. A **confusion matrix** is a table where each cell $[i,j]$ indicates how often label $j$ was predicted when the correct label was $i$. Thus, the diagonal entries (i.e., cells $[i,i]$) indicate labels that were correctly predicted, and the off-diagonal entries indicate errors. In the following example, we generate a confusion matrix for the bigram tagger developed in Chapter 5:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from pickle import load\r\n",
    "input = open('t2.pkl', 'rb')\r\n",
    "t2 = load(input)\r\n",
    "input.close()\r\n",
    "def tag_list(tagged_sents):\r\n",
    "    return [tag for sent in tagged_sents for (word, tag) in sent]\r\n",
    "def apply_tagger(tagger, corpus):\r\n",
    "    return [tagger.tag(nltk.tag.untag(sent)) for sent in corpus]\r\n",
    "gold = tag_list(brown.tagged_sents(categories='editorial'))\r\n",
    "test = tag_list(apply_tagger(t2, brown.tagged_sents(categories='editorial')))\r\n",
    "cm = nltk.ConfusionMatrix(gold, test)\r\n",
    "print(cm.pretty_format(sort_by_count=True, show_percents=True, truncate=9))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The confusion matrix indicates that common errors include a substitution of `NN` for `JJ` (for 1.6% of words), and of `NN` for `NNS` (for 1.5% of words). Note that periods (.) indicate cells whose value is 0, and that the diagonal entries — which correspond to correct classifications — are marked with angle brackets."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.5   Cross-Validation\n",
    "\n",
    "In order to evaluate our models, we must reserve a portion of the annotated data for the test set. As we already mentioned, if the test set is too small, then our evaluation may not be accurate. However, making the test set larger usually means making the training set smaller, which can have a significant impact on performance if a limited amount of annotated data is available.\n",
    "\n",
    "One solution to this problem is to perform multiple evaluations on different test sets, then to combine the scores from those evaluations, a technique known as **cross-validation**. In particular, we subdivide the original corpus into *N* subsets called **folds**. For each of these folds, we train a model using all of the data *except* the data in that fold, and then test that model on the fold. Even though the individual folds might be too small to give accurate evaluation scores on their own, the combined evaluation score is based on a large amount of data, and is therefore quite reliable.\n",
    "\n",
    "A second, and equally important, advantage of using cross-validation is that it allows us to examine how widely the performance varies across different training sets. If we get very similar scores for all *N* training sets, then we can be fairly confident that the score is accurate. On the other hand, if scores vary widely across the *N* training sets, then we should probably be skeptical about the accuracy of the evaluation score."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 4   Decision Trees\n",
    "\n",
    "In the next three sections, we'll take a closer look at three machine learning methods that can be used to automatically build classification models: decision trees, naive Bayes classifiers, and Maximum Entropy classifiers. As we've seen, it's possible to treat these learning methods as black boxes, simply training models and using them for prediction without understanding how they work. But there's a lot to be learned from taking a closer look at how these learning methods select models based on the data in a training set. An understanding of these methods can help guide our selection of appropriate features, and especially our decisions about how those features should be encoded. And an understanding of the generated models can allow us to extract information about which features are most informative, and how those features relate to one another.\n",
    "\n",
    "A **decision tree** is a simple flowchart that selects labels for input values. This flowchart consists of **decision nodes**, which check feature values, and **leaf nodes**, which assign labels. To choose the label for an input value, we begin at the flowchart's initial decision node, known as its **root node**. This node contains a condition that checks one of the input value's features, and selects a branch based on that feature's value. Following the branch that describes our input value, we arrive at a new decision node, with a new condition on the input value's features. We continue following the branch selected by each node's condition, until we arrive at a leaf node which provides a label for the input value. The following figure shows an example decision tree model for the name gender task.\n",
    "\n",
    "<img src='images/decision-tree.png' width='500' />\n",
    "\n",
    "*Decision Tree model for the name gender task. Note that tree diagrams are conventionally drawn \"upside down,\" with the root at the top, and the leaves at the bottom.*\n",
    "\n",
    "Once we have a decision tree, it is straightforward to use it to assign labels to new input values. What's less straightforward is how we can build a decision tree that models a given training set. But before we look at the learning algorithm for building decision trees, we'll consider a simpler task: picking the best \"decision stump\" for a corpus. A **decision stump** is a decision tree with a single node that decides how to classify inputs based on a single feature. It contains one leaf for each possible feature value, specifying the class label that should be assigned to inputs whose features have that value. In order to build a decision stump, we must first decide which feature should be used. The simplest method is to just build a decision stump for each possible feature, and see which one achieves the highest accuracy on the training data, although there are other alternatives that we will discuss below. Once we've picked a feature, we can build the decision stump by assigning a label to each leaf based on the most frequent label for the selected examples in the training set (i.e., the examples where the selected feature has that value).\n",
    "\n",
    "Given the algorithm for choosing decision stumps, the algorithm for growing larger decision trees is straightforward. We begin by selecting the overall best decision stump for the classification task. We then check the accuracy of each of the leaves on the training set. Leaves that do not achieve sufficient accuracy are then replaced by new decision stumps, trained on the subset of the training corpus that is selected by the path to the leaf. For example, we could grow the decision tree from above by replacing the leftmost leaf with a new decision stump, trained on the subset of the training set names that do not start with a \"k\" or end with a vowel or an \"l.\""
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 4.1   Entropy and Information Gain\n",
    "\n",
    "As was mentioned before, there are several methods for identifying the most informative feature for a decision stump. One popular alternative, called **information gain**, measures how much more organized the input values become when we divide them up using a given feature. To measure how disorganized the original set of input values are, we calculate entropy of their labels, which will be high if the input values have highly varied labels, and low if many input values all have the same label. In particular, entropy is defined as the sum of the probability of each label times the log probability of that same label:\n",
    "\n",
    "(1)\t\t$H = −Σ_{l \\in labels}P(l) × \\log_2P(l)$.\n",
    "\n",
    "<img src='images/Binary_entropy_plot.png' width='200' />\n",
    "\n",
    "*The entropy of labels in the name gender prediction task, as a function of the percentage of names in a given set that are male.*\n",
    "\n",
    "For example, the figure shows how the entropy of labels in the name gender prediction task depends on the ratio of male to female names. Note that if most input values have the same label (e.g., if P(male) is near 0 or near 1), then entropy is low. In particular, labels that have low frequency do not contribute much to the entropy (since $P(l)$ is small), and labels with high frequency also do not contribute much to the entropy (since $\\log_2P(l)$ is small). On the other hand, if the input values have a wide variety of labels, then there are many labels with a \"medium\" frequency, where neither $P(l)$ nor $\\log_2P(l)$ is small, so the entropy is high. The following code demonstrates how to calculate the entropy of a list of labels."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import math\r\n",
    "def entropy(labels):\r\n",
    "    freqdist = nltk.FreqDist(labels)\r\n",
    "    probs = [freqdist.freq(l) for l in freqdist]\r\n",
    "    return -sum(p * math.log(p,2) for p in probs)\r\n",
    "\r\n",
    "print(entropy(['male', 'male', 'male', 'male'])) "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print(entropy(['male', 'female', 'male', 'male']))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print(entropy(['female', 'male', 'female', 'male']))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print(entropy(['female', 'female', 'male', 'female']))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print(entropy(['female', 'female', 'female', 'female'])) "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Once we have calculated the entropy of the original set of input values' labels, we can determine how much more organized the labels become once we apply the decision stump. To do so, we calculate the entropy for each of the decision stump's leaves, and take the average of those leaf entropy values (weighted by the number of samples in each leaf). The information gain is then equal to the original entropy minus this new, reduced entropy. The higher the information gain, the better job the decision stump does of dividing the input values into coherent groups, so we can build decision trees by selecting the decision stumps with the highest information gain.\n",
    "\n",
    "Another consideration for decision trees is efficiency. The simple algorithm for selecting decision stumps described above must construct a candidate decision stump for every possible feature, and this process must be repeated for every node in the constructed decision tree. A number of algorithms have been developed to cut down on the training time by storing and reusing information about previously evaluated examples.\n",
    "\n",
    "Decision trees have a number of useful qualities. To begin with, they're simple to understand, and easy to interpret. This is especially true near the top of the decision tree, where it is usually possible for the learning algorithm to find very useful features. Decision trees are especially well suited to cases where many hierarchical categorical distinctions can be made. For example, decision trees can be very effective at capturing phylogeny trees.\n",
    "\n",
    "However, decision trees also have a few disadvantages. One problem is that, since each branch in the decision tree splits the training data, the amount of training data available to train nodes lower in the tree can become quite small. As a result, these lower decision nodes may **overfit** the training set, learning patterns that reflect idiosyncrasies of the training set rather than linguistically significant patterns in the underlying problem. One solution to this problem is to stop dividing nodes once the amount of training data becomes too small. Another solution is to grow a full decision tree, but then to **prune** decision nodes that do not improve performance on a dev-test.\n",
    "\n",
    "A second problem with decision trees is that they force features to be checked in a specific order, even when features may act relatively independently of one another. For example, when classifying documents into topics (such as sports, automotive, or murder mystery), features such as `hasword(football)` are highly indicative of a specific label, regardless of what the other feature values are. Since there is limited space near the top of the decision tree, most of these features will need to be repeated on many different branches in the tree. And since the number of branches increases exponentially as we go down the tree, the amount of repetition can be very large.\n",
    "\n",
    "A related problem is that decision trees are not good at making use of features that are weak predictors of the correct label. Since these features make relatively small incremental improvements, they tend to occur very low in the decision tree. But by the time the decision tree learner has descended far enough to use these features, there is not enough training data left to reliably determine what effect they should have. If we could instead look at the effect of these features across the entire training set, then we might be able to make some conclusions about how they should affect the choice of label.\n",
    "\n",
    "The fact that decision trees require that features be checked in a specific order limits their ability to exploit features that are relatively independent of one another. The naive Bayes classification method, which we'll discuss next, overcomes this limitation by allowing all features to act \"in parallel.\""
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 5   Naive Bayes Classifiers\n",
    "\n",
    "In **naive Bayes** classifiers, every feature gets a say in determining which label should be assigned to a given input value. To choose a label for an input value, the naive Bayes classifier begins by calculating the **prior probability** of each label, which is determined by checking frequency of each label in the training set. The contribution from each feature is then combined with this prior probability, to arrive at a likelihood estimate for each label. The label whose likelihood estimate is the highest is then assigned to the input value. The following figure illustrates this process.\n",
    "\n",
    "<img src='images/naive-bayes-triangle.png' width='300' />\n",
    "\n",
    "*An abstract illustration of the procedure used by the naive Bayes classifier to choose the topic for a document. In the training corpus, most documents are automotive, so the classifier starts out at a point closer to the \"automotive\" label. But it then considers the effect of each feature. In this example, the input document contains the word \"dark,\" which is a weak indicator for murder mysteries, but it also contains the word \"football,\" which is a strong indicator for sports documents. After every feature has made its contribution, the classifier checks which label it is closest to, and assigns that label to the input.*\n",
    "\n",
    "Individual features make their contribution to the overall decision by \"voting against\" labels that don't occur with that feature very often. In particular, the likelihood score for each label is reduced by multiplying it by the probability that an input value with that label would have the feature. For example, if the word *run* occurs in 12% of the sports documents, 10% of the murder mystery documents, and 2% of the automotive documents, then the likelihood score for the sports label will be multiplied by 0.12; the likelihood score for the murder mystery label will be multiplied by 0.1, and the likelihood score for the automotive label will be multiplied by 0.02. The overall effect will be to reduce the score of the murder mystery label slightly more than the score of the sports label, and to significantly reduce the automotive label with respect to the other two labels. This process is illustrated in the next two figures.\n",
    "\n",
    "<img src='images/naive_bayes_bargraph.png' width='500' />\n",
    "\n",
    "*Calculating label likelihoods with naive Bayes. Naive Bayes begins by calculating the prior probability of each label, based on how frequently each label occurs in the training data. Every feature then contributes to the likelihood estimate for each label, by multiplying it by the probability that input values with that label will have that feature. The resulting likelihood score can be thought of as an estimate of the probability that a randomly selected value from the training set would have both the given label and the set of features, assuming that the feature probabilities are all independent.*\n",
    "\n",
    "### 5.1   Underlying Probabilistic Model\n",
    "\n",
    "Another way of understanding the naive Bayes classifier is that it chooses the most likely label for an input, under the assumption that every input value is generated by first choosing a class label for that input value, and then generating each feature, entirely independent of every other feature. Of course, this assumption is unrealistic; features are often highly dependent on one another. We'll return to some of the consequences of this assumption at the end of this section. This simplifying assumption, known as the **naive Bayes assumption** (or **independence assumption**) makes it much easier to combine the contributions of the different features, since we don't need to worry about how they should interact with one another.\n",
    "\n",
    "<img src='images/naive_bayes_graph.png' width='300' />\n",
    "\n",
    "*A Bayesian Network Graph illustrating the generative process that is assumed by the naive Bayes classifier. To generate a labeled input, the model first chooses a label for the input, then it generates each of the input's features based on that label. Every feature is assumed to be entirely independent of every other feature, given the label.*\n",
    "\n",
    "Based on this assumption, we can calculate an expression for $P(label|features)$, the probability that an input will have a particular label given that it has a particular set of features. To choose a label for a new input, we can then simply pick the label $l$ that maximizes $P(l|features)$.\n",
    "\n",
    "To begin, we note that $P(label|features)$ is equal to the probability that an input has a particular label *and* the specified set of features, divided by the probability that it has the specified set of features:\n",
    "\n",
    "(2)\t\t$P(label|features) = P(features, label)/P(features)$\n",
    "\n",
    "Next, we note that $P(features)$ will be the same for every choice of label, so if we are simply interested in finding the most likely label, it suffices to calculate $P(features, label)$, which we'll call the label likelihood.\n",
    "\n",
    "If we want to generate a probability estimate for each label, rather than just choosing the most likely label, then the easiest way to compute $P(features)$ is to simply calculate the sum over labels $l$ of $P(features, l)$:\n",
    "\n",
    "(3)\t\t$P(features) = Σ_{l \\in labels} P(features, l)$\n",
    "\n",
    "The label likelihood can be expanded out as the probability of the label times the probability of the features given the label:\n",
    "\n",
    "(4)\t\t$P(features, label) = P(label) × P(features|label)$\n",
    "\n",
    "Furthermore, since the features are all independent of one another (given the label), we can separate out the probability of each individual feature:\n",
    "\n",
    "(5)\t\t$P(features, label) = P(label) × \\Pi_{f \\in features}P(f|label)$\n",
    "\n",
    "This is exactly the equation we discussed above for calculating the label likelihood: $P(label)$ is the prior probability for a given label, and each $P(f|label)$ is the contribution of a single feature to the label likelihood.\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 5.2   Zero Counts and Smoothing\n",
    "\n",
    "The simplest way to calculate $P(f|label)$, the contribution of a feature $f$ toward the label likelihood for a label $label$, is to take the percentage of training instances with the given label that also have the given feature:\n",
    "\n",
    "(6)\t\t$P(f|label) = count(f, label) / count(label)$\n",
    "\n",
    "However, this simple approach can become problematic when a feature *never* occurs with a given label in the training set. In this case, our calculated value for $P(f|label)$ will be zero, which will cause the label likelihood for the given label to be zero. Thus, the input will never be assigned this label, regardless of how well the other features fit the label.\n",
    "\n",
    "The basic problem here is with our calculation of $P(f|label)$, the probability that an input will have a feature, given a label. In particular, just because we haven't seen a feature/label combination occur in the training set, doesn't mean it's impossible for that combination to occur. For example, we may not have seen any murder mystery documents that contained the word \"football,\" but we wouldn't want to conclude that it's completely impossible for such documents to exist.\n",
    "\n",
    "Thus, although $count(f,label)/count(label)$ is a good estimate for $P(f|label)$ when $count(f, label)$ is relatively high, this estimate becomes less reliable when $count(f)$ becomes smaller. Therefore, when building naive Bayes models, we usually employ more sophisticated techniques, known as **smoothing** techniques, for calculating $P(f|label)$, the probability of a feature given a label. For example, the **Expected Likelihood Estimation** for the probability of a feature given a label basically adds 0.5 to each $count(f,label)$ value, and the **Heldout Estimation** uses a heldout corpus to calculate the relationship between feature frequencies and feature probabilities. The `nltk.probability` module provides support for a wide variety of smoothing techniques."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 5.3   Non-Binary Features\n",
    "\n",
    "We have assumed here that each feature is binary, i.e. that each input either has a feature or does not. Label-valued features (e.g., a color feature which could be red, green, blue, white, or orange) can be converted to binary features by replacing them with binary features such as \"color-is-red\". Numeric features can be converted to binary features by **binning**, which replaces them with features such as \"4<x<6\".\n",
    "\n",
    "Another alternative is to use regression methods to model the probabilities of numeric features. For example, if we assume that the height feature has a bell curve distribution, then we could estimate P(height|label) by finding the mean and variance of the heights of the inputs with each label. In this case, $P(f=v|label)$ would not be a fixed value, but would vary depending on the value of $v$.\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 5.4   The Naivete of Independence\n",
    "\n",
    "The reason that naive Bayes classifiers are called \"naive\" is that it's unreasonable to assume that all features are independent of one another (given the label). In particular, almost all real-world problems contain features with varying degrees of dependence on one another. If we had to avoid any features that were dependent on one another, it would be very difficult to construct good feature sets that provide the required information to the machine learning algorithm.\n",
    "\n",
    "So what happens when we ignore the independence assumption, and use the naive Bayes classifier with features that are not independent? One problem that arises is that the classifier can end up \"double-counting\" the effect of highly correlated features, pushing the classifier closer to a given label than is justified.\n",
    "\n",
    "To see how this can occur, consider a name gender classifier that contains two identical features, $f_1$ and $f_2$. In other words, $f_2$ is an exact copy of $f_1$, and contains no new information. When the classifier is considering an input, it will include the contribution of both $f_1$ and $f_2$ when deciding which label to choose. Thus, the information content of these two features will be given more weight than it deserves.\n",
    "\n",
    "Of course, we don't usually build naive Bayes classifiers that contain two identical features. However, we do build classifiers that contain features which are dependent on one another. For example, the features `ends-with(a)` and `ends-with(vowel)` are dependent on one another, because if an input value has the first feature, then it must also have the second feature. For features like these, the duplicated information may be given more weight than is justified by the training set.\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 5.5   The Cause of Double-Counting\n",
    "\n",
    "The reason for the double-counting problem is that during training, feature contributions are computed separately; but when using the classifier to choose labels for new inputs, those feature contributions are combined. One solution, therefore, is to consider the possible interactions between feature contributions during training. We could then use those interactions to adjust the contributions that individual features make.\n",
    "\n",
    "To make this more precise, we can rewrite the equation used to calculate the likelihood of a label, separating out the contribution made by each feature (or label):\n",
    "\n",
    "(7)\t\t$P(features, label) = w[label] × \\Pi_{f \\in features} w[f, label]$\n",
    "\n",
    "Here, $w[label]$ is the \"starting score\" for a given label, and $w[f, label]$ is the contribution made by a given feature towards a label's likelihood. We call these values $w[label]$ and $w[f, label]$ the **parameters** or **weights** for the model. Using the naive Bayes algorithm, we set each of these parameters independently:\n",
    "\n",
    "(8)\t\t$w[label] = P(label)$\n",
    "\n",
    "(9)\t\t$w[f, label] = P(f|label)$\n",
    "\n",
    "However, in the next section, we'll look at a classifier that considers the possible interactions between these parameters when choosing their values.\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 6   Maximum Entropy Classifiers\n",
    "\n",
    "The **Maximum Entropy** classifier uses a model that is very similar to the model employed by the naive Bayes classifier. But rather than using probabilities to set the model's parameters, it uses search techniques to find a set of parameters that will maximize the performance of the classifier. In particular, it looks for the set of parameters that maximizes the **total likelihood** of the training corpus, which is defined as:\n",
    "\n",
    "(10)\t\t$P(features) = Σ_{x \\in \\textrm{corpus}} P(label(x)|features(x))$\n",
    "\n",
    "Where `P(label|features)`, the probability that an input whose features are `features` will have class label `label`, is defined as:\n",
    "\n",
    "(11)\t\t$P(label|features) = P(label, features) / Σ_{l \\in labels} P(l, features)$\n",
    "\n",
    "Because of the potentially complex interactions between the effects of related features, there is no way to directly calculate the model parameters that maximize the likelihood of the training set. Therefore, Maximum Entropy classifiers choose the model parameters using **iterative optimization** techniques, which initialize the model's parameters to random values, and then repeatedly refine those parameters to bring them closer to the optimal solution. These iterative optimization techniques guarantee that each refinement of the parameters will bring them closer to the optimal values, but do not necessarily provide a means of determining when those optimal values have been reached. Because the parameters for Maximum Entropy classifiers are selected using iterative optimization techniques, they can take a long time to learn. This is especially true when the size of the training set, the number of features, and the number of labels are all large."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 6.1   The Maximum Entropy Model\n",
    "\n",
    "The Maximum Entropy classifier model is a generalization of the model used by the naive Bayes classifier. Like the naive Bayes model, the Maximum Entropy classifier calculates the likelihood of each label for a given input value by multiplying together the parameters that are applicable for the input value and label. The naive Bayes classifier model defines a parameter for each label, specifying its prior probability, and a parameter for each (feature, label) pair, specifying the contribution of individual features towards a label's likelihood.\n",
    "\n",
    "In contrast, the Maximum Entropy classifier model leaves it up to the user to decide what combinations of labels and features should receive their own parameters. In particular, it is possible to use a single parameter to associate a feature with more than one label; or to associate more than one feature with a given label. This will sometimes allow the model to \"generalize\" over some of the differences between related labels or features.\n",
    "\n",
    "Each combination of labels and features that receives its own parameter is called a **joint-feature**. Note that joint-features are properties of *labeled* values, whereas (simple) features are properties of *unlabeled* values.\n",
    "\n",
    "Typically, the joint-features that are used to construct Maximum Entropy models exactly mirror those that are used by the naive Bayes model. In particular, a joint-feature is defined for each label, corresponding to $w[label]$, and for each combination of (simple) feature and label, corresponding to $w[f,label]$. Given the joint-features for a Maximum Entropy model, the score assigned to a label for a given input is simply the product of the parameters associated with the joint-features $j$ that apply to that input and label:\n",
    "\n",
    "(12)\t\t$P(input, label) = \\Pi_{j \\in joint\\textrm{-}features(input,label)} w[j]$"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 6.2   Maximizing Entropy\r\n",
    "\r\n",
    "The intuition that motivates Maximum Entropy classification is that we should build a model that captures the frequencies of individual joint-features, without making any unwarranted assumptions. An example will help to illustrate this principle.\r\n",
    "\r\n",
    "Suppose we are assigned the task of picking the correct word sense for a given word, from a list of ten possible senses (labeled A-J). At first, we are not told anything more about the word or the senses. There are many probability distributions that we could choose for the ten senses, such as:\r\n",
    "\r\n",
    "|\t   |     A| \t  B|    C| \t  D|   \t  E|   \t  F|   \tG| \t  H| \tI|  \t J|\r\n",
    "|-----:|-----:|-------:|----:|----:|------:|------:|----:|----:|----:|-------:|\r\n",
    "|*(i)* |   10%| \t10%|  10%| \t10%| \t10%| \t10%|  10%| \t10%|  10%| \t   10%|\r\n",
    "|*(ii)*|    5%|\t    15%|   0%| \t30%| \t 0%| \t 8%|  12%| \t 0%|   6%| \t   24%|\r\n",
    "|*(iii)*|   0%|    100%|   0%| \t 0%| \t 0%|  \t 0%|   0%| \t 0%|   0%| \t    0%|\r\n",
    "\r\n",
    "Although any of these distributions *might* be correct, we are likely to choose distribution *(i)*, because without any more information, there is no reason to believe that any word sense is more likely than any other. On the other hand, distributions *(ii)* and *(iii)* reflect assumptions that are not supported by what we know.\r\n",
    "\r\n",
    "One way to capture this intuition that distribution *(i)* is more \"fair\" than the other two is to invoke the concept of entropy. In the discussion of decision trees, we described entropy as a measure of how \"disorganized\" a set of labels was. In particular, if a single label dominates then entropy is low, but if the labels are more evenly distributed then entropy is high. In our example, we chose distribution *(i)* because its label probabilities are evenly distributed — in other words, because its entropy is high. In general, the **Maximum Entropy principle** states that, among the distributions that are consistent with what we know, we should choose the distribution whose entropy is highest.\r\n",
    "\r\n",
    "Next, suppose that we are told that sense A appears 55% of the time. Once again, there are many distributions that are consistent with this new piece of information, such as:\r\n",
    "\r\n",
    "\r\n",
    "|       |     A| \t  B|   \t C|   \t D|  \t E|  \t F|  \t G|  \t H|  \t I|  \t J|\r\n",
    "|------:|-----:|------:|-----:|------:|------:|------:|------:|------:|------:|------:|\r\n",
    "|*(iv)* | \t55%| \t45%| \t0%| \t0%| \t0%| \t0%| \t0%| \t0%| \t0%| \t0%|\r\n",
    "|*(v)* \t|   55%| \t 5%| \t5%| \t5%| \t5%| \t5%| \t5%| \t5%| \t5%| \t5%|\r\n",
    "|*(vi)* |\t55%| \t 3%| \t1%| \t2%| \t9%| \t5%| \t0%|    25%| \t0%| \t0%|\r\n",
    "\r\n",
    "But again, we will likely choose the distribution that makes the fewest unwarranted assumptions — in this case, distribution *(v)*.\r\n",
    "\r\n",
    "Finally, suppose that we are told that the word \"up\" appears in the nearby context 10% of the time, and that when it does appear in the context there's an 80% chance that sense A or C will be used. In this case, we will have a harder time coming up with an appropriate distribution by hand; however, we can verify that the following distribution looks appropriate:\r\n",
    "\r\n",
    "|\t  \t|      |        A|   \tB|  \tC|  \tD|  \tE| \t    F|  \tG| \t    H|  \tI|  \tJ|\r\n",
    "|------:|-----:|--------:|------:|------:|------:|------:|------:|------:|------:|------:|------:|\r\n",
    "|*(vii)*| \t+up| \t 5.1%| \t0.25%| \t 2.9%| \t0.25%| \t0.25%| \t0.25%| \t0.25%| \t0.25%| \t0.25%| \t0.25%|\r\n",
    "|       | \t-up| \t49.9%| \t4.46%| \t4.46%| \t4.46%| \t4.46%| \t4.46%| \t4.46%| \t4.46%| \t4.46%| \t4.46%|\r\n",
    "\r\n",
    "In particular, the distribution is consistent with what we know: if we add up the probabilities in column A, we get 55%; if we add up the probabilities of row 1, we get 10%; and if we add up the boxes for senses A and C in the +up row, we get 8% (or 80% of the +up cases). Furthermore, the remaining probabilities appear to be \"evenly distributed.\"\r\n",
    "\r\n",
    "Throughout this example, we have restricted ourselves to distributions that are consistent with what we know; among these, we chose the distribution with the highest entropy. This is exactly what the Maximum Entropy classifier does as well. In particular, for each joint-feature, the Maximum Entropy model calculates the \"empirical frequency\" of that feature — i.e., the frequency with which it occurs in the training set. It then searches for the distribution which maximizes entropy, while still predicting the correct frequency for each joint-feature.\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 6.3   Generative vs Conditional Classifiers\n",
    "\n",
    "An important difference between the naive Bayes classifier and the Maximum Entropy classifier concerns the type of questions they can be used to answer. The naive Bayes classifier is an example of a **generative** classifier, which builds a model that predicts $P(input, label)$, the joint probability of a $(input, label)$ pair. As a result, generative models can be used to answer the following questions:\n",
    "\n",
    "1.    What is the most likely label for a given input?\n",
    "2.    How likely is a given label for a given input?\n",
    "3.    What is the most likely input value?\n",
    "4.    How likely is a given input value?\n",
    "5.    How likely is a given input value with a given label?\n",
    "6.    What is the most likely label for an input that might have one of two values (but we don't know which)?\n",
    "\n",
    "The Maximum Entropy classifier, on the other hand, is an example of a **conditional** classifier. Conditional classifiers build models that predict $P(label|input)$ — the probability of a label given the input value. Thus, conditional models can still be used to answer questions 1 and 2. However, conditional models can not be used to answer the remaining questions 3-6.\n",
    "\n",
    "In general, generative models are strictly more powerful than conditional models, since we can calculate the conditional probability $P(label|input)$ from the joint probability $P(input, label)$, but not vice versa. However, this additional power comes at a price. Because the model is more powerful, it has more \"free parameters\" which need to be learned. However, the size of the training set is fixed. Thus, when using a more powerful model, we end up with less data that can be used to train each parameter's value, making it harder to find the best parameter values. As a result, a generative model may not do as good a job at answering questions 1 and 2 as a conditional model, since the conditional model can focus its efforts on those two questions. However, if we do need answers to questions like 3-6, then we have no choice but to use a generative model.\n",
    "\n",
    "The difference between a generative model and a conditional model is analogous to the difference between a topographical map and a picture of a skyline. Although the topographical map can be used to answer a wider variety of questions, it is significantly more difficult to generate an accurate topographical map than it is to generate an accurate skyline."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 7   Modeling Linguistic Patterns\n",
    "\n",
    "Classifiers can help us to understand the linguistic patterns that occur in natural language, by allowing us to create explicit **models** that capture those patterns. Typically, these models are using supervised classification techniques, but it is also possible to build analytically motivated models. Either way, these explicit models serve two important purposes: they help us to understand linguistic patterns, and they can be used to make predictions about new language data.\n",
    "\n",
    "The extent to which explicit models can give us insights into linguistic patterns depends largely on what kind of model is used. Some models, such as decision trees, are relatively transparent, and give us direct information about which factors are important in making decisions and about which factors are related to one another. Other models, such as multi-level neural networks, are much more opaque. Although it can be possible to gain insight by studying them, it typically takes a lot more work.\n",
    "\n",
    "But all explicit models can make predictions about new \"**unseen**\" language data that was not included in the corpus used to build the model. These predictions can be evaluated to assess the accuracy of the model. Once a model is deemed sufficiently accurate, it can then be used to automatically predict information about new language data. These predictive models can be combined into systems that perform many useful language processing tasks, such as document classification, automatic translation, and question answering.\n",
    "\n",
    "### 7.1   What do models tell us?\n",
    "\n",
    "It's important to understand what we can learn about language from an automatically constructed model. One important consideration when dealing with models of language is the distinction between descriptive models and explanatory models. Descriptive models capture patterns in the data but they don't provide any information about *why* the data contains those patterns. For example, as we saw in Chapter 3, the synonyms *absolutely* and *definitely* are not interchangeable: we say *absolutely adore* not *definitely adore*, and *definitely prefer* not *absolutely prefer*. In contrast, explanatory models attempt to capture properties and relationships that cause the linguistic patterns. For example, we might introduce the abstract concept of \"polar verb\", as one that has an extreme meaning, and categorize some verb like *adore* and *detest* as polar. Our explanatory model would contain the constraint that *absolutely* can only combine with polar verbs, and *definitely* can only combine with non-polar verbs. In summary, descriptive models provide information about correlations in the data, while explanatory models go further to postulate causal relationships.\n",
    "\n",
    "Most models that are automatically constructed from a corpus are descriptive models; in other words, they can tell us what features are relevant to a given pattern or construction, but they can't necessarily tell us how those features and patterns relate to one another. If our goal is to understand the linguistic patterns, then we can use this information about which features are related as a starting point for further experiments designed to tease apart the relationships between features and patterns. On the other hand, if we're just interested in using the model to make predictions (e.g., as part of a language processing system), then we can use the model to make predictions about new data without worrying about the details of underlying causal relationships.\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 8   Summary\n",
    "\n",
    "-    Modeling the linguistic data found in corpora can help us to understand linguistic patterns, and can be used to make predictions about new language data.\n",
    "-    Supervised classifiers use labeled training corpora to build models that predict the label of an input based on specific features of that input.\n",
    "-    Supervised classifiers can perform a wide variety of NLP tasks, including document classification, part-of-speech tagging, sentence segmentation, dialogue act type identification, and determining entailment relations, and many other tasks.\n",
    "-    When training a supervised classifier, you should split your corpus into three datasets: a training set for building the classifier model; a dev-test set for helping select and tune the model's features; and a test set for evaluating the final model's performance.\n",
    "-    When evaluating a supervised classifier, it is important that you use fresh data, that was not included in the training or dev-test set. Otherwise, your evaluation results may be unrealistically optimistic.\n",
    "-    Decision trees are automatically constructed tree-structured flowcharts that are used to assign labels to input values based on their features. Although they're easy to interpret, they are not very good at handling cases where feature values interact in determining the proper label.\n",
    "-    In naive Bayes classifiers, each feature independently contributes to the decision of which label should be used. This allows feature values to interact, but can be problematic when two or more features are highly correlated with one another.\n",
    "-    Maximum Entropy classifiers use a basic model that is similar to the model used by naive Bayes; however, they employ iterative optimization to find the set of feature weights that maximizes the probability of the training set.\n",
    "-    Most of the models that are automatically constructed from a corpus are descriptive — they let us know which features are relevant to a given pattern or construction, but they don't give any information about causal relationships between those features and patterns.\n",
    "\n"
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.7 64-bit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "interpreter": {
   "hash": "7691dc7e968cae2c1305b7cd748cf3c51f27e2f981106cfee5b02ab727978e13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}