{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# 7. Extracting Information from Text\r\n",
    "\r\n",
    "For any given question, it's likely that someone has written the answer down somewhere. The amount of natural language text that is available in electronic form is truly staggering, and is increasing every day. However, the complexity of natural language can make it very difficult to access the information in that text. The state of the art in NLP is still a long way from being able to build general-purpose representations of meaning from unrestricted text. If we instead focus our efforts on a limited set of questions or \"entity relations,\" such as \"where are different facilities located,\" or \"who is employed by what company,\" we can make significant progress. The goal of this chapter is to answer the following questions:\r\n",
    "\r\n",
    "1.    How can we build a system that extracts structured data, such as tables, from unstructured text?\r\n",
    "2.    What are some robust methods for identifying the entities and relationships described in a text?\r\n",
    "3.    Which corpora are appropriate for this work, and how do we use them for training and evaluating our models?\r\n",
    "\r\n",
    "Along the way, we'll apply techniques from the last two chapters to the problems of chunking and named-entity recognition.\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1   Information Extraction\n",
    "\n",
    "Information comes in many shapes and sizes. One important form is **structured data**, where there is a regular and predictable organization of entities and relationships. For example, we might be interested in the relation between companies and locations. Given a particular company, we would like to be able to identify the locations where it does business; conversely, given a location, we would like to discover which companies do business in that location. If our data is in tabular form, such as the following example, then answering these queries is straightforward:\n",
    "\n",
    "*Locations data*\n",
    "\n",
    "|OrgName \t          |LocationName|\n",
    "|:--------------------|:-----------|\n",
    "|Omnicom \t          |New York    |\n",
    "|DDB Needham \t      |New York    |\n",
    "|Kaplan Thaler Group  |New York    |\n",
    "|BBDO South \t      |Atlanta     |\n",
    "|Georgia-Pacific \t  |Atlanta     |\n",
    "\n",
    "If this location data was stored in Python as a list of tuples `(entity, relation, entity)`, then the question \"Which organizations operate in Atlanta?\" could be translated as follows:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "locs = [('Omnicom', 'IN', 'New York'),\r\n",
    "        ('DDB Needham', 'IN', 'New York'),\r\n",
    "        ('Kaplan Thaler Group', 'IN', 'New York'),\r\n",
    "        ('BBDO South', 'IN', 'Atlanta'),\r\n",
    "        ('Georgia-Pacific', 'IN', 'Atlanta')]\r\n",
    "query = [e1 for (e1, rel, e2) in locs if e2=='Atlanta']\r\n",
    "print(query)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['BBDO South', 'Georgia-Pacific']\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "*Companies that operate in Atlanta*\n",
    "\n",
    "|OrgName        |\n",
    "|:--------------|\n",
    "|BBDO South     |\n",
    "|Georgia-Pacific|\n",
    "\n",
    "Things are more tricky if we try to get similar information out of text. For example, consider the following snippet (from `nltk.corpus.ieer`, for fileid `NYT19980315.0085`):\n",
    "\n",
    "> (1) The fourth Wells account moving to another agency is the packaged paper-products division of Georgia-Pacific Corp., which arrived at Wells only last fall. Like Hertz and the History Channel, it is also leaving for an Omnicom-owned agency, the BBDO South unit of BBDO Worldwide. BBDO South in Atlanta, which handles corporate advertising for Georgia-Pacific, will assume additional duties for brands like Angel Soft toilet tissue and Sparkle paper towels, said Ken Haldin, a spokesman for Georgia-Pacific in Atlanta.\n",
    "\n",
    "If you read through (1), you will glean the information required to answer the example question. But how do we get a machine to understand enough about (1) to return the answers in the previous table? This is obviously a much harder task. Unlike the first table, (1) contains no structure that links organization names with location names.\n",
    "\n",
    "One approach to this problem involves building a very general representation of meaning (see Chapter 10). In this chapter we take a different approach, deciding in advance that we will only look for very specific kinds of information in text, such as the relation between organizations and locations. Rather than trying to use text like (1) to answer the question directly, we first convert the **unstructured data** of natural language sentences into the structured data of the first table. Then we reap the benefits of powerful query tools such as SQL. This method of getting meaning from text is called **Information Extraction**.\n",
    "\n",
    "Information Extraction has many applications, including business intelligence, resume harvesting, media analysis, sentiment detection, patent search, and email scanning. A particularly important area of current research involves the attempt to extract structured data out of electronically-available scientific literature, especially in the domain of biology and medicine."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1.1   Information Extraction Architecture\n",
    "\n",
    "The following figure shows the architecture for a simple information extraction system. It begins by processing a document using several of the procedures discussed in Chapter 3 and 5: first, the raw text of the document is split into sentences using a sentence segmenter, and each sentence is further subdivided into words using a tokenizer. Next, each sentence is tagged with part-of-speech tags, which will prove very helpful in the next step, **named entity detection**. In this step, we search for mentions of potentially interesting entities in each sentence. Finally, we use **relation detection** to search for likely relations between different entities in the text.\n",
    "\n",
    "<img src='images/ie-architecture.png' width='400' />\n",
    "\n",
    "*Simple Pipeline Architecture for an Information Extraction System. This system takes the raw text of a document as its input, and generates a list of (entity, relation, entity) tuples as its output. For example, given a document that indicates that the company Georgia-Pacific is located in Atlanta, it might generate the tuple `([ORG: 'Georgia-Pacific'] 'in' [LOC: 'Atlanta'])`.*\n",
    "\n",
    "To perform the first three tasks, we can define a simple function that simply connects together NLTK's default sentence segmenter, word tokenizer, and part-of-speech tagger:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "def ie_preprocess(document):\r\n",
    "   sentences = nltk.sent_tokenize(document)\r\n",
    "   sentences = [nltk.word_tokenize(sent) for sent in sentences]\r\n",
    "   sentences = [nltk.pos_tag(sent) for sent in sentences]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Next, in named entity detection, we segment and label the entities that might participate in interesting relations with one another. Typically, these will be definite noun phrases such as *the knights who say \"ni\"*, or proper names such as *Monty Python*. In some tasks it is useful to also consider indefinite nouns or noun chunks, such as *every student* or *cats*, and these do not necessarily refer to entities in the same way as definite `NP`s and proper names.\n",
    "\n",
    "Finally, in relation extraction, we search for specific patterns between pairs of entities that occur near one another in the text, and use those patterns to build tuples recording the relationships between the entities."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2   Chunking\n",
    "\n",
    "The basic technique we will use for entity detection is **chunking**, which segments and labels multi-token sequences as illustrated below. The smaller boxes show the word-level tokenization and part-of-speech tagging, while the large boxes show higher-level chunking. Each of these larger boxes is called a **chunk**. Like tokenization, which omits whitespace, chunking usually selects a subset of the tokens. Also like tokenization, the pieces produced by a chunker do not overlap in the source text.\n",
    "\n",
    "<img src='images/chunk-segmentation.png' width='500' />\n",
    "\n",
    "*Segmentation and Labeling at both the Token and Chunk Levels*\n",
    "\n",
    "In this section, we will explore chunking in some depth, beginning with the definition and representation of chunks. We will see regular expression and n-gram approaches to chunking, and will develop and evaluate chunkers using the CoNLL-2000 chunking corpus. We will then return in Section 5 and 6 to the tasks of named entity recognition and relation extraction."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2.1   Noun Phrase Chunking\n",
    "\n",
    "We will begin by considering the task of **noun phrase chunking**, or **NP-chunking**, where we search for chunks corresponding to individual noun phrases. For example, here is some Wall Street Journal text with `NP`-chunks marked using brackets:\n",
    "\n",
    "    (2) [ The/DT market/NN ] for/IN [ system-management/NN software/NN ] for/IN \n",
    "        [  Digital/NNP ] [ 's/POS hardware/NN ] is/VBZ fragmented/JJ enough/RB that/IN \n",
    "        [ a/DT giant/NN ] such/JJ as/IN [ Computer/NNP Associates/NNPS ] should/MD do/VB \n",
    "          well/RB there/RB ./.\n",
    "\n",
    "As we can see, `NP`-chunks are often smaller pieces than complete noun phrases. For example, *the market for system-management software for Digital's hardware* is a single noun phrase (containing two nested noun phrases), but it is captured in `NP`-chunks by the simpler chunk *the market*. One of the motivations for this difference is that `NP`-chunks are defined so as not to contain other `NP`-chunks. Consequently, any prepositional phrases or subordinate clauses that modify a nominal will not be included in the corresponding `NP`-chunk, since they almost certainly contain further noun phrases.\n",
    "\n",
    "One of the most useful sources of information for `NP`-chunking is part-of-speech tags. This is one of the motivations for performing part-of-speech tagging in our information extraction system. We demonstrate this approach using an example sentence that has been part-of-speech tagged in the following code. In order to create an `NP`-chunker, we will first define a **chunk grammar**, consisting of rules that indicate how sentences should be chunked. In this case, we will define a simple grammar with a single regular-expression rule. This rule says that an NP chunk should be formed whenever the chunker finds an optional determiner (`DT`) followed by any number of adjectives (`JJ`) and then a noun (`NN`). Using this grammar, we create a chunk parser, and test it on our example sentence. The result is a tree, which we can either print, or display graphically."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "import nltk\r\n",
    "sentence = [(\"the\", \"DT\"), (\"little\", \"JJ\"), (\"yellow\", \"JJ\"), \r\n",
    "(\"dog\", \"NN\"), (\"barked\", \"VBD\"), (\"at\", \"IN\"),  (\"the\", \"DT\"), (\"cat\", \"NN\")]\r\n",
    "\r\n",
    "grammar = \"NP: {<DT>?<JJ>*<NN>}\"\r\n",
    "\r\n",
    "cp = nltk.RegexpParser(grammar)\r\n",
    "result = cp.parse(sentence)\r\n",
    "print(result)\r\n",
    "#display(result)\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(S\n",
      "  (NP the/DT little/JJ yellow/JJ dog/NN)\n",
      "  barked/VBD\n",
      "  at/IN\n",
      "  (NP the/DT cat/NN))\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2.2   Tag Patterns\r\n",
    "\r\n",
    "The rules that make up a chunk grammar use **tag patterns** to describe sequences of tagged words. A tag pattern is a sequence of part-of-speech tags delimited using angle brackets, e.g. `<DT>?<JJ>*<NN>`. Tag patterns are similar to regular expression patterns (Chapter 3). Now, consider the following noun phrases from the Wall Street Journal:\r\n",
    "\r\n",
    "    another/DT sharp/JJ dive/NN\r\n",
    "    trade/NN figures/NNS\r\n",
    "    any/DT new/JJ policy/NN measures/NNS\r\n",
    "    earlier/JJR stages/NNS\r\n",
    "    Panamanian/JJ dictator/NN Manuel/NNP Noriega/NNP\r\n",
    "\r\n",
    "We can match these noun phrases using a slight refinement of the first tag pattern above, i.e. `<DT>?<JJ.*>*<NN.*>+`. This will chunk any sequence of tokens beginning with an optional determiner, followed by zero or more adjectives of any type (including relative adjectives like `earlier/JJR`), followed by one or more nouns of any type. However, it is easy to find many more complicated examples which this rule will not cover:\r\n",
    "\r\n",
    "    his/PRP$ Mansion/NNP House/NNP speech/NN\r\n",
    "    the/DT price/NN cutting/VBG\r\n",
    "    3/CD %/NN to/TO 4/CD %/NN\r\n",
    "    more/JJR than/IN 10/CD %/NN\r\n",
    "    the/DT fastest/JJS developing/VBG trends/NNS\r\n",
    "    's/POS skill/NN\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2.3   Chunking with Regular Expressions\n",
    "\n",
    "To find the chunk structure for a given sentence, the `RegexpParser` chunker begins with a flat structure in which no tokens are chunked. The chunking rules are applied in turn, successively updating the chunk structure. Once all of the rules have been invoked, the resulting chunk structure is returned.\n",
    "\n",
    "The following listing shows a simple chunk grammar consisting of two rules. The first rule matches an optional determiner or possessive pronoun, zero or more adjectives, then a noun. The second rule matches one or more proper nouns. We also define an example sentence to be chunked, and run the chunker on this input."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "grammar = r\"\"\"\r\n",
    "  NP: {<DT|PP\\$>?<JJ>*<NN>}   # chunk determiner/possessive, adjectives and noun\r\n",
    "      {<NNP>+}                # chunk sequences of proper nouns\r\n",
    "\"\"\"\r\n",
    "cp = nltk.RegexpParser(grammar)\r\n",
    "sentence = [(\"Rapunzel\", \"NNP\"), (\"let\", \"VBD\"), (\"down\", \"RP\"), \r\n",
    "                 (\"her\", \"PP$\"), (\"long\", \"JJ\"), (\"golden\", \"JJ\"), (\"hair\", \"NN\")]\r\n",
    "\r\n",
    "print(cp.parse(sentence))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(S\n",
      "  (NP Rapunzel/NNP)\n",
      "  let/VBD\n",
      "  down/RP\n",
      "  (NP her/PP$ long/JJ golden/JJ hair/NN))\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "If a tag pattern matches at overlapping locations, the leftmost match takes precedence. For example, if we apply a rule that matches two consecutive nouns to a text containing three consecutive nouns, then only the first two nouns will be chunked:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "nouns = [(\"money\", \"NN\"), (\"market\", \"NN\"), (\"fund\", \"NN\")]\r\n",
    "grammar = \"NP: {<NN><NN>}  # Chunk two consecutive nouns\"\r\n",
    "cp = nltk.RegexpParser(grammar)\r\n",
    "print(cp.parse(nouns))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(S (NP money/NN market/NN) fund/NN)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Once we have created the chunk for *money market*, we have removed the context that would have permitted *fund* to be included in a chunk. This issue would have been avoided with a more permissive chunk rule, e.g. `NP: {<NN>+}`."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2.4   Exploring Text Corpora\n",
    "\n",
    "In Chapter 5 we saw how we could interrogate a tagged corpus to extract phrases matching a particular sequence of part-of-speech tags. We can do the same work more easily with a chunker, as follows:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "cp = nltk.RegexpParser('CHUNK: {<V.*> <TO> <V.*>}')\r\n",
    "brown = nltk.corpus.brown\r\n",
    "for sent in brown.tagged_sents():\r\n",
    "    tree = cp.parse(sent)\r\n",
    "    for subtree in tree.subtrees():\r\n",
    "        if subtree.label() == 'CHUNK': print(subtree)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2.5   Chinking\n",
    "\n",
    "Sometimes it is easier to define what we want to *exclude* from a chunk. We can define a **chink** to be a sequence of tokens that is not included in a chunk. In the following example, `barked/VBD at/IN` is a chink:\n",
    "\n",
    "    [ the/DT little/JJ yellow/JJ dog/NN ] barked/VBD at/IN [ the/DT cat/NN ]\n",
    "\n",
    "Chinking is the process of removing a sequence of tokens from a chunk. If the matching sequence of tokens spans an entire chunk, then the whole chunk is removed; if the sequence of tokens appears in the middle of the chunk, these tokens are removed, leaving two chunks where there was only one before. If the sequence is at the periphery of the chunk, these tokens are removed, and a smaller chunk remains. These three possibilities are illustrated below.\n",
    "\n",
    "*Three chinking rules applied to the same chunk*\n",
    "\n",
    "|          |                     Entire chunk| \t        Middle of a chunk|                 End of a chunk|\n",
    "|---------:|--------------------------------:|--------------------------:|------------------------------:|\n",
    "|Input     |`[a/DT little/JJ dog/NN]` \t     |`[a/DT little/JJ dog/NN]`  |\t    `[a/DT little/JJ dog/NN]`|\n",
    "|Operation |\tChink \"DT JJ NN\" \t         |Chink \"JJ\" \t             |Chink \"NN\"                     |\n",
    "|Pattern   |`}DT JJ NN{`                     | \t`}JJ{` \t                 |  `}NN{`                       |\n",
    "|Output    |`a/DT little/JJ dog/NN`          |`[a/DT] little/JJ [dog/NN]`| `[a/DT little/JJ] dog/NN`     |\n",
    "\n",
    "We put the entire sentence into a single chunk, then excise the chinks."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "grammar = r\"\"\"\r\n",
    "  NP:\r\n",
    "    {<.*>+}          # Chunk everything\r\n",
    "    }<VBD|IN>+{      # Chink sequences of VBD and IN\r\n",
    "  \"\"\"\r\n",
    "sentence = [(\"the\", \"DT\"), (\"little\", \"JJ\"), (\"yellow\", \"JJ\"),\r\n",
    "       (\"dog\", \"NN\"), (\"barked\", \"VBD\"), (\"at\", \"IN\"),  (\"the\", \"DT\"), (\"cat\", \"NN\")]\r\n",
    "cp = nltk.RegexpParser(grammar)\r\n",
    "print(cp.parse(sentence))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2.6   Representing Chunks: Tags vs Trees\r\n",
    "\r\n",
    "As befits their intermediate status between tagging and parsing (Chapter 8), chunk structures can be represented using either tags or trees. The most widespread file representation uses **IOB tags**. In this scheme, each token is tagged with one of three special chunk tags, `I` (inside), `O` (outside), or `B` (begin). A token is tagged as `B` if it marks the beginning of a chunk. Subsequent tokens within the chunk are tagged `I`. All other tokens are tagged `O`. The `B` and `I` tags are suffixed with the chunk type, e.g. `B-NP`, `I-NP`. Of course, it is not necessary to specify a chunk type for tokens that appear outside a chunk, so these are just labeled `O`. \r\n",
    "\r\n",
    "<img src= 'images/chunk-tagrep.png' width='400' />\r\n",
    "\r\n",
    "*Tag Representation of Chunk Structures*\r\n",
    "\r\n",
    "IOB tags have become the standard way to represent chunk structures in files, and we will also be using this format. Here is how the information from above would appear in a file:\r\n",
    "\r\n",
    "    We PRP B-NP\r\n",
    "    saw VBD O\r\n",
    "    the DT B-NP\r\n",
    "    yellow JJ I-NP\r\n",
    "    dog NN I-NP\r\n",
    "\r\n",
    "In this representation there is one token per line, each with its part-of-speech tag and chunk tag. This format permits us to represent more than one chunk type, so long as the chunks do not overlap. As we saw earlier, chunk structures can also be represented using trees. These have the benefit that each chunk is a constituent that can be manipulated directly. An example is shown below:\r\n",
    "\r\n",
    "<img src='images/chunk-treerep.png' width='400' />\r\n",
    "\r\n",
    "*Tree Representation of Chunk Structures*\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3   Developing and Evaluating Chunkers\r\n",
    "\r\n",
    "Now you have a taste of what chunking does, but we haven't explained how to evaluate chunkers. As usual, this requires a suitably annotated corpus. We begin by looking at the mechanics of converting IOB format into an NLTK tree, then at how this is done on a larger scale using a chunked corpus. We will see how to score the accuracy of a chunker relative to a corpus, then look at some more data-driven ways to search for NP chunks. Our focus throughout will be on expanding the coverage of a chunker.\r\n",
    "\r\n",
    "### 3.1   Reading IOB Format and the CoNLL 2000 Corpus\r\n",
    "\r\n",
    "Using the corpus module we can load Wall Street Journal text that has been tagged then chunked using the IOB notation. The chunk categories provided in this corpus are `NP`, `VP` and `PP`. As we have seen, each sentence is represented using multiple lines, as shown below:\r\n",
    "\r\n",
    "    he PRP B-NP\r\n",
    "    accepted VBD B-VP\r\n",
    "    the DT B-NP\r\n",
    "    position NN I-NP\r\n",
    "    ...\r\n",
    "\r\n",
    "A conversion function `chunk.conllstr2tree()` builds a tree representation from one of these multi-line strings. Moreover, it permits us to choose any subset of the three chunk types to use, here just for `NP` chunks:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "text = '''\r\n",
    "he PRP B-NP\r\n",
    "accepted VBD B-VP\r\n",
    "the DT B-NP\r\n",
    "position NN I-NP\r\n",
    "of IN B-PP\r\n",
    "vice NN B-NP\r\n",
    "chairman NN I-NP\r\n",
    "of IN B-PP\r\n",
    "Carlyle NNP B-NP\r\n",
    "Group NNP I-NP\r\n",
    ", , O\r\n",
    "a DT B-NP\r\n",
    "merchant NN I-NP\r\n",
    "banking NN I-NP\r\n",
    "concern NN I-NP\r\n",
    ". . O\r\n",
    "'''\r\n",
    "tree = nltk.chunk.conllstr2tree(text, chunk_types=['NP'])\r\n",
    "print(tree)\r\n",
    "display(tree)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can use the NLTK corpus module to access a larger amount of chunked text. The CoNLL 2000 corpus contains 270k words of Wall Street Journal text, divided into \"train\" and \"test\" portions, annotated with part-of-speech tags and chunk tags in the IOB format. We can access the data using `nltk.corpus.conll2000`. Here is an example that reads the 100th sentence of the \"train\" portion of the corpus:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from nltk.corpus import conll2000\r\n",
    "print(conll2000.chunked_sents('train.txt')[99])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "As you can see, the CoNLL 2000 corpus contains three chunk types: `NP` chunks, which we have already seen; `VP` chunks such as *has already delivered*; and `PP` chunks such as *because of*. Since we are only interested in the `NP` chunks right now, we can use the `chunk_types` argument to select them:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print(conll2000.chunked_sents('train.txt', chunk_types=['NP'])[99])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.2   Simple Evaluation and Baselines\n",
    "\n",
    "Now that we can access a chunked corpus, we can evaluate chunkers. We start off by establishing a baseline for the trivial chunk parser `cp` that creates no chunks:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from nltk.corpus import conll2000\r\n",
    "cp = nltk.RegexpParser(\"\")\r\n",
    "test_sents = conll2000.chunked_sents('test.txt', chunk_types=['NP'])\r\n",
    "print(cp.evaluate(test_sents))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The IOB tag accuracy indicates that more than a third of the words are tagged with `O`, i.e. not in an `NP` chunk. However, since our tagger did not find *any* chunks, its precision, recall, and f-measure are all zero. Now let's try a naive regular expression chunker that looks for tags beginning with letters that are characteristic of noun phrase tags (e.g. `CD`, `DT`, and `JJ`)."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "grammar = r\"NP: {<[CDJNP].*>+}\"\r\n",
    "cp = nltk.RegexpParser(grammar)\r\n",
    "print(cp.evaluate(test_sents))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "As you can see, this approach achieves decent results. However, we can improve on it by adopting a more data-driven approach, where we use the training corpus to find the chunk tag (`I`, `O`, or `B`) that is most likely for each part-of-speech tag. In other words, we can build a chunker using a *unigram tagger*. But rather than trying to determine the correct part-of-speech tag for each word, we are trying to determine the correct chunk tag, given each word's part-of-speech tag.\n",
    "\n",
    "In the following listing, we define the `UnigramChunker` class, which uses a unigram tagger to label sentences with chunk tags. Most of the code in this class is simply used to convert back and forth between the chunk tree representation used by NLTK's `ChunkParserI` interface, and the IOB representation used by the embedded tagger. The class defines two methods: a constructor which is called when we build a new UnigramChunker; and the `parse` method which is used to chunk new sentences."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "class UnigramChunker(nltk.ChunkParserI):\r\n",
    "    def __init__(self, train_sents): \r\n",
    "        train_data = [[(t,c) for w,t,c in nltk.chunk.tree2conlltags(sent)]\r\n",
    "                      for sent in train_sents]\r\n",
    "        self.tagger = nltk.UnigramTagger(train_data) \r\n",
    "\r\n",
    "    def parse(self, sentence): \r\n",
    "        pos_tags = [pos for (word,pos) in sentence]\r\n",
    "        tagged_pos_tags = self.tagger.tag(pos_tags)\r\n",
    "        chunktags = [chunktag for (pos, chunktag) in tagged_pos_tags]\r\n",
    "        conlltags = [(word, pos, chunktag) for ((word,pos),chunktag)\r\n",
    "                     in zip(sentence, chunktags)]\r\n",
    "        return nltk.chunk.conlltags2tree(conlltags)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The constructor expects a list of training sentences, which will be in the form of chunk trees. It first converts training data to a form that is suitable for training the tagger, using `tree2conlltags` to map each chunk tree to a list of `word,tag,chunk` triples. It then uses that converted training data to train a unigram tagger, and stores it in `self.tagger` for later use.\n",
    "\n",
    "The `parse` method takes a tagged sentence as its input, and begins by extracting the part-of-speech tags from that sentence. It then tags the part-of-speech tags with IOB chunk tags, using the tagger `self.tagger` that was trained in the constructor. Next, it extracts the chunk tags, and combines them with the original sentence, to yield `conlltags`. Finally, it uses `conlltags2tree` to convert the result back into a chunk tree.\n",
    "\n",
    "Now that we have `UnigramChunker`, we can train it using the CoNLL 2000 corpus, and test its resulting performance:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "test_sents = conll2000.chunked_sents('test.txt', chunk_types=['NP'])\r\n",
    "train_sents = conll2000.chunked_sents('train.txt', chunk_types=['NP'])\r\n",
    "unigram_chunker = UnigramChunker(train_sents)\r\n",
    "print(unigram_chunker.evaluate(test_sents))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "This chunker does reasonably well, achieving an overall f-measure score of 83%. Let's take a look at what it's learned, by using its unigram tagger to assign a tag to each of the part-of-speech tags that appear in the corpus:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "postags = sorted(set(pos for sent in train_sents\r\n",
    "                     for (word,pos) in sent.leaves()))\r\n",
    "print(unigram_chunker.tagger.tag(postags))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "It has discovered that most punctuation marks occur outside of `NP` chunks, with the exception of `#` and `$`, both of which are used as currency markers. It has also found that determiners (`DT`) and possessives (`PRP$` and `WP$`) occur at the beginnings of `NP` chunks, while noun types (`NN`, `NNP`, `NNPS`, `NNS`) mostly occur inside of `NP` chunks.\r\n",
    "\r\n",
    "Having built a unigram chunker, it is quite easy to build a bigram chunker: we simply change the class name to `BigramChunker`, and assign `nltk.BigramTagger(train_data)` to `self.tagger`. The resulting chunker has slightly higher performance than the unigram chunker:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "class BigramChunker(nltk.ChunkParserI):\r\n",
    "    def __init__(self, train_sents): \r\n",
    "        train_data = [[(t,c) for w,t,c in nltk.chunk.tree2conlltags(sent)]\r\n",
    "                      for sent in train_sents]\r\n",
    "        self.tagger = nltk.BigramTagger(train_data) \r\n",
    "\r\n",
    "    def parse(self, sentence): \r\n",
    "        pos_tags = [pos for (word,pos) in sentence]\r\n",
    "        tagged_pos_tags = self.tagger.tag(pos_tags)\r\n",
    "        chunktags = [chunktag for (pos, chunktag) in tagged_pos_tags]\r\n",
    "        conlltags = [(word, pos, chunktag) for ((word,pos),chunktag)\r\n",
    "                     in zip(sentence, chunktags)]\r\n",
    "        return nltk.chunk.conlltags2tree(conlltags)  \t\r\n",
    "\r\n",
    "bigram_chunker = BigramChunker(train_sents)\r\n",
    "print(bigram_chunker.evaluate(test_sents))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.3   Training Classifier-Based Chunkers\r\n",
    "\r\n",
    "Both the regular-expression based chunkers and the n-gram chunkers decide what chunks to create entirely based on part-of-speech tags. However, sometimes part-of-speech tags are insufficient to determine how a sentence should be chunked. For example, consider the following two statements:\r\n",
    "\t\r\n",
    "    (3a)\t\tJoey/NN sold/VBD the/DT farmer/NN rice/NN ./.\r\n",
    "\r\n",
    "    (3b)\t\tNick/NN broke/VBD my/DT computer/NN monitor/NN ./.\r\n",
    "\r\n",
    "These two sentences have the same part-of-speech tags, yet they are chunked differently. In the first sentence, *the farmer* and *rice* are separate chunks, while the corresponding material in the second sentence, *the computer monitor*, is a single chunk. Clearly, we need to make use of information about the content of the words, in addition to just their part-of-speech tags, if we wish to maximize chunking performance.\r\n",
    "\r\n",
    "One way that we can incorporate information about the content of words is to use a classifier-based tagger to chunk the sentence. Like the n-gram chunker considered in the previous section, this classifier-based chunker will work by assigning IOB tags to the words in a sentence, and then converting those tags to chunks. For the classifier-based tagger itself, we will use the same approach that we used in Chapter 6 to build a part-of-speech tagger.\r\n",
    "\r\n",
    "The basic code for the classifier-based NP chunker is shown below. It consists of two classes. The first class is almost identical to the `ConsecutivePosTagger` class from Chapter 6. The only two differences are that it calls a different feature extractor and that it uses a MaxentClassifier rather than a NaiveBayesClassifier. The second class is basically a wrapper around the tagger class that turns it into a chunker. During training, this second class maps the chunk trees in the training corpus into tag sequences; in the `parse()` method, it converts the tag sequence provided by the tagger back into a chunk tree."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "class ConsecutiveNPChunkTagger(nltk.TaggerI): \r\n",
    "\r\n",
    "    def __init__(self, train_sents):\r\n",
    "        train_set = []\r\n",
    "        for tagged_sent in train_sents:\r\n",
    "            untagged_sent = nltk.tag.untag(tagged_sent)\r\n",
    "            history = []\r\n",
    "            for i, (word, tag) in enumerate(tagged_sent):\r\n",
    "                featureset = npchunk_features(untagged_sent, i, history) \r\n",
    "                train_set.append( (featureset, tag) )\r\n",
    "                history.append(tag)\r\n",
    "        self.classifier = nltk.MaxentClassifier.train( \r\n",
    "            train_set, algorithm='megam', trace=0)\r\n",
    "\r\n",
    "    def tag(self, sentence):\r\n",
    "        history = []\r\n",
    "        for i, word in enumerate(sentence):\r\n",
    "            featureset = npchunk_features(sentence, i, history)\r\n",
    "            tag = self.classifier.classify(featureset)\r\n",
    "            history.append(tag)\r\n",
    "        return zip(sentence, history)\r\n",
    "\r\n",
    "class ConsecutiveNPChunker(nltk.ChunkParserI): \r\n",
    "    def __init__(self, train_sents):\r\n",
    "        tagged_sents = [[((w,t),c) for (w,t,c) in\r\n",
    "                         nltk.chunk.tree2conlltags(sent)]\r\n",
    "                        for sent in train_sents]\r\n",
    "        self.tagger = ConsecutiveNPChunkTagger(tagged_sents)\r\n",
    "\r\n",
    "    def parse(self, sentence):\r\n",
    "        tagged_sents = self.tagger.tag(sentence)\r\n",
    "        conlltags = [(w,t,c) for ((w,t),c) in tagged_sents]\r\n",
    "        return nltk.chunk.conlltags2tree(conlltags)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The only piece left to fill in is the feature extractor. We begin by defining a simple feature extractor which just provides the part-of-speech tag of the current token. Using this feature extractor, our classifier-based chunker is very similar to the unigram chunker, as is reflected in its performance:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def npchunk_features(sentence, i, history):\r\n",
    "    word, pos = sentence[i]\r\n",
    "    return {\"pos\": pos}\r\n",
    "chunker = ConsecutiveNPChunker(train_sents)\r\n",
    "print(chunker.evaluate(test_sents))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can also add a feature for the previous part-of-speech tag. Adding this feature allows the classifier to model interactions between adjacent tags, and results in a chunker that is closely related to the bigram chunker."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def npchunk_features(sentence, i, history):\r\n",
    "    word, pos = sentence[i]\r\n",
    "    if i == 0:\r\n",
    "        prevword, prevpos = \"<START>\", \"<START>\"\r\n",
    "    else:\r\n",
    "        prevword, prevpos = sentence[i-1]\r\n",
    "    return {\"pos\": pos, \"prevpos\": prevpos}\r\n",
    "chunker = ConsecutiveNPChunker(train_sents)\r\n",
    "print(chunker.evaluate(test_sents))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Next, we'll try adding a feature for the current word, since we hypothesized that word content should be useful for chunking. We find that this feature does indeed improve the chunker's performance, by about 1.5 percentage points (which corresponds to about a 10% reduction in the error rate)."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def npchunk_features(sentence, i, history):\r\n",
    "    word, pos = sentence[i]\r\n",
    "    if i == 0:\r\n",
    "        prevword, prevpos = \"<START>\", \"<START>\"\r\n",
    "    else:\r\n",
    "        prevword, prevpos = sentence[i-1]\r\n",
    "    return {\"pos\": pos, \"word\": word, \"prevpos\": prevpos}\r\n",
    "chunker = ConsecutiveNPChunker(train_sents)\r\n",
    "print(chunker.evaluate(test_sents))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Finally, we can try extending the feature extractor with a variety of additional features, such as lookahead features, paired features, and complex contextual features. The last feature, called `tags-since-dt`, creates a string describing the set of all part-of-speech tags that have been encountered since the most recent determiner, or since the beginning of the sentence if there is no determiner before index `i`."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def npchunk_features(sentence, i, history):\r\n",
    "    word, pos = sentence[i]\r\n",
    "    if i == 0:\r\n",
    "        prevword, prevpos = \"<START>\", \"<START>\"\r\n",
    "    else:\r\n",
    "        prevword, prevpos = sentence[i-1]\r\n",
    "    if i == len(sentence)-1:\r\n",
    "        nextword, nextpos = \"<END>\", \"<END>\"\r\n",
    "    else:\r\n",
    "        nextword, nextpos = sentence[i+1]\r\n",
    "    return {\"pos\": pos,\r\n",
    "            \"word\": word,\r\n",
    "            \"prevpos\": prevpos,\r\n",
    "            \"nextpos\": nextpos, \r\n",
    "            \"prevpos+pos\": \"%s+%s\" % (prevpos, pos),  \r\n",
    "            \"pos+nextpos\": \"%s+%s\" % (pos, nextpos),\r\n",
    "            \"tags-since-dt\": tags_since_dt(sentence, i)}  \r\n",
    "\r\n",
    "def tags_since_dt(sentence, i):\r\n",
    "    tags = set()\r\n",
    "    for word, pos in sentence[:i]:\r\n",
    "        if pos == 'DT':\r\n",
    "            tags = set()\r\n",
    "        else:\r\n",
    "            tags.add(pos)\r\n",
    "    return '+'.join(sorted(tags))\r\n",
    "\r\n",
    "chunker = ConsecutiveNPChunker(train_sents)\r\n",
    "print(chunker.evaluate(test_sents))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 4   Recursion in Linguistic Structure\n",
    "\n",
    "### 4.1   Building Nested Structure with Cascaded Chunkers\n",
    "\n",
    "So far, our chunk structures have been relatively flat. Trees consist of tagged tokens, optionally grouped under a chunk node such as `NP`. However, it is possible to build chunk structures of arbitrary depth, simply by creating a multi-stage chunk grammar containing recursive rules. The following listing has patterns for noun phrases, prepositional phrases, verb phrases, and sentences. This is a four-stage chunk grammar, and can be used to create structures having a depth of at most four."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "grammar = r\"\"\"\r\n",
    "  CLAUSE: {<NP><VP>}           # Chunk NP, VP\r\n",
    "  NP: {<DT|JJ|NN.*>+}          # Chunk sequences of DT, JJ, NN\r\n",
    "  PP: {<IN><NP>}               # Chunk prepositions followed by NP\r\n",
    "  VP: {<VB.*><NP|PP|CLAUSE>+$} # Chunk verbs and their arguments\r\n",
    "  \"\"\"\r\n",
    "cp = nltk.RegexpParser(grammar)\r\n",
    "sentence = [(\"Mary\", \"NN\"), (\"saw\", \"VBD\"), (\"the\", \"DT\"), (\"cat\", \"NN\"),\r\n",
    "    (\"sit\", \"VB\"), (\"on\", \"IN\"), (\"the\", \"DT\"), (\"mat\", \"NN\")]\r\n",
    "\r\n",
    "print(cp.parse(sentence))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Unfortunately this result misses the `VP` headed by *saw*. The solution to is to get the chunker to loop over its patterns: after trying all of them, it repeats the process. We add an optional second argument `loop` to specify the number of times the set of patterns should be run:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "cp = nltk.RegexpParser(grammar, loop=2)\r\n",
    "print(cp.parse(sentence))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 4.2   Trees\n",
    "\n",
    "A **tree** is a set of connected labeled nodes, each reachable by a unique path from a distinguished root node. Here's an example of a tree (note that they are standardly drawn upside-down):\n",
    "\n",
    "(4) <img src='images/ch07-tree-3.png' width='200' />\n",
    "\n",
    "We use a 'family' metaphor to talk about the relationships of nodes in a tree: for example, `S` is the **parent** of `VP`; conversely `VP` is a **child** of `S`. Also, since `NP` and `VP` are both children of `S`, they are also **siblings**. For convenience, there is also a text format for specifying trees:\n",
    "  \t\n",
    "\n",
    "    (S\n",
    "       (NP Alice)\n",
    "       (VP\n",
    "          (V chased)\n",
    "          (NP\n",
    "             (Det the)\n",
    "             (N rabbit))))\n",
    "\n",
    "Although we will focus on syntactic trees, trees can be used to encode any homogeneous hierarchical structure that spans a sequence of linguistic forms (e.g. morphological structure, discourse structure). In the general case, leaves and node values do not have to be strings.\n",
    "\n",
    "In NLTK, we create a tree by giving a node label and a list of children:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "tree1 = nltk.Tree('NP', ['Alice'])\r\n",
    "print(tree1)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(NP Alice)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "tree2 = nltk.Tree('NP', ['the', 'rabbit'])\r\n",
    "print(tree2)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(NP the rabbit)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can incorporate these into successively larger trees as follows:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "tree3 = nltk.Tree('VP', ['chased', tree2])\r\n",
    "tree4 = nltk.Tree('S', [tree1, tree3])\r\n",
    "print(tree4)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(S (NP Alice) (VP chased (NP the rabbit)))\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Here are some of the methods available for tree objects:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "print(tree4[1])"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(VP chased (NP the rabbit))\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "tree4[1].label()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'VP'"
      ]
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "tree4.leaves()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['Alice', 'chased', 'the', 'rabbit']"
      ]
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "tree4[1][1][1]"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'rabbit'"
      ]
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "display(tree3)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 4.3   Tree Traversal\n",
    "\n",
    "It is standard to use a recursive function to traverse a tree:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "def traverse(t):\r\n",
    "    try:\r\n",
    "        t.label()\r\n",
    "    except AttributeError:\r\n",
    "        print(t, end=\" \")\r\n",
    "    else:\r\n",
    "        # Now we know that t.node is defined\r\n",
    "        print('(', t.label(), end=\" \")\r\n",
    "        for child in t:\r\n",
    "            traverse(child)\r\n",
    "        print(')', end=\" \")\r\n",
    "\r\n",
    "t = nltk.Tree.fromstring('(S (NP Alice) (VP chased (NP the rabbit)))')\r\n",
    "traverse(t)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "( S ( NP Alice ) ( VP chased ( NP the rabbit ) ) ) "
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "We have used a technique called **duck typing** to detect that `t` is a tree (i.e. `t.label()` is defined)."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 5   Named Entity Recognition\n",
    "\n",
    "At the start of this chapter, we briefly introduced named entities (NEs). Named entities are definite noun phrases that refer to specific types of individuals, such as organizations, persons, dates, and so on. The following table lists some of the more commonly used types of NEs. These should be self-explanatory, except for \"Facility\": human-made artifacts in the domains of architecture and civil engineering; and \"GPE\": geo-political entities such as city, state/province, and country.\n",
    "\n",
    "*Commonly Used Types of Named Entity*\n",
    "\n",
    "|NE Type \t    |Examples                                |\n",
    "|:--------------|:---------------------------------------|\n",
    "|ORGANIZATION \t|Georgia-Pacific Corp., WHO              |\n",
    "|PERSON \t    |Eddy Bonte, President Obama             |\n",
    "|LOCATION \t    |Murray River, Mount Everest             |\n",
    "|DATE \t        |June, 2008-06-29                        |\n",
    "|TIME \t        |two fifty a m, 1:30 p.m.                |\n",
    "|MONEY \t        |175 million Canadian Dollars, GBP 10.40 |\n",
    "|PERCENT \t    |twenty pct, 18.75 %                     |\n",
    "|FACILITY \t    |Washington Monument, Stonehenge         |\n",
    "|GPE \t        |South East Asia, Midlothian             |\n",
    "\n",
    "The goal of a **named entity recognition** (NER) system is to identify all textual mentions of the named entities. This can be broken down into two sub-tasks: identifying the boundaries of the NE, and identifying its type. While named entity recognition is frequently a prelude to identifying relations in Information Extraction, it can also contribute to other tasks. For example, in Question Answering (QA), we try to improve the precision of Information Retrieval by recovering not whole pages, but just those parts which contain an answer to the user's question. Most QA systems take the documents returned by standard Information Retrieval, and then attempt to isolate the minimal text snippet in the document containing the answer. Now suppose the question was *Who was the first President of the US?*, and one of the documents that was retrieved contained the following passage:\n",
    "\n",
    "(5)\n",
    ">The Washington Monument is the most prominent structure in Washington, D.C. and one of the city's early attractions. It was built in honor of George Washington, who led the country to independence and then became its first President.\n",
    "\n",
    "Analysis of the question leads us to expect that an answer should be of the form *X was the first President of the US*, where *X* is not only a noun phrase, but also refers to a named entity of type PERSON. This should allow us to ignore the first sentence in the passage. While it contains two occurrences of *Washington*, named entity recognition should tell us that neither of them has the correct type.\n",
    "\n",
    "How do we go about identifying named entities? One option would be to look up each word in an appropriate list of names. For example, in the case of locations, we could use a **gazetteer**, or geographical dictionary, such as the Alexandria Gazetteer or the Getty Gazetteer. However, doing this blindly runs into problems, as shown below:\n",
    "\n",
    "<img src='images/locations.png' width='500' />\n",
    "\n",
    "*Location Detection by Simple Lookup for a News Story: Looking up every word in a gazetteer is error-prone; case distinctions may help, but these are not always present.*\n",
    "\n",
    "Observe that the gazetteer has good coverage of locations in many countries, and incorrectly finds locations like Sanchez in the Dominican Republic and On in Vietnam. Of course we could omit such locations from the gazetteer, but then we won't be able to identify them when they do appear in a document.\n",
    "\n",
    "It gets even harder in the case of names for people or organizations. Any list of such names will probably have poor coverage. New organizations come into existence every day, so if we are trying to deal with contemporary newswire or blog entries, it is unlikely that we will be able to recognize many of the entities using gazetteer lookup.\n",
    "\n",
    "Another major source of difficulty is caused by the fact that many named entity terms are ambiguous. Thus *May* and *North* are likely to be parts of named entities for DATE and LOCATION, respectively, but could both be part of a PERSON; conversely *Christian Dior* looks like a PERSON but is more likely to be of type ORGANIZATION. A term like *Yankee* will be ordinary modifier in some contexts, but will be marked as an entity of type ORGANIZATION in the phrase *Yankee infielders*.\n",
    "\n",
    "Further challenges are posed by multi-word names like *Stanford University*, and by names that contain other names such as *Cecil H. Green Library* and *Escondido Village Conference Service Center*. In named entity recognition, therefore, we need to be able to identify the beginning and end of multi-token sequences.\n",
    "\n",
    "Named entity recognition is a task that is well-suited to the type of classifier-based approach that we saw for noun phrase chunking. In particular, we can build a tagger that labels each word in a sentence using the IOB format, where chunks are labeled by their appropriate type. Here is part of the CONLL 2002 (`conll2002`) Dutch training data:\n",
    "\n",
    "    Eddy N B-PER\n",
    "    Bonte N I-PER\n",
    "    is V O\n",
    "    woordvoerder N O\n",
    "    van Prep O\n",
    "    diezelfde Pron O\n",
    "    Hogeschool N B-ORG\n",
    "    . Punc O\n",
    "\n",
    "In this representation, there is one token per line, each with its part-of-speech tag and its named entity tag. Based on this training corpus, we can construct a tagger that can be used to label new sentences; and use the `nltk.chunk.conlltags2tree()` function to convert the tag sequences into a chunk tree.\n",
    "\n",
    "NLTK provides a classifier that has already been trained to recognize named entities, accessed with the function `nltk.ne_chunk()`. If we set the parameter `binary=True`, then named entities are just tagged as `NE`; otherwise, the classifier adds category labels such as PERSON, ORGANIZATION, and GPE."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "sent = nltk.corpus.treebank.tagged_sents()[22]\n",
    "print(nltk.ne_chunk(sent, binary=True))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print(nltk.ne_chunk(sent)) "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 6   Relation Extraction\n",
    "\n",
    "Once named entities have been identified in a text, we then want to extract the relations that exist between them. As indicated earlier, we will typically be looking for relations between specified types of named entity. One way of approaching this task is to initially look for all triples of the form (*X*, , *Y*), where *X* and *Y* are named entities of the required types, and  is the string of words that intervenes between *X* and *Y*. We can then use regular expressions to pull out just those instances of  that express the relation that we are looking for. The following example searches for strings that contain the word *in*. The special regular expression `(?!\\b.+ing\\b)` is a negative lookahead assertion that allows us to disregard strings such as *success in supervising the transition of*, where *in* is followed by a gerund."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import re\n",
    "IN = re.compile(r'.*\\bin\\b(?!\\b.+ing)')\n",
    "for doc in nltk.corpus.ieer.parsed_docs('NYT_19980315'):\n",
    "    for rel in nltk.sem.extract_rels('ORG', 'LOC', doc,\n",
    "                                     corpus='ieer', pattern = IN):\n",
    "        print(nltk.sem.rtuple(rel))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Searching for the keyword *in* works reasonably well, though it will also retrieve false positives such as `[ORG: House Transportation Committee] , secured the most money in the [LOC: New York]`; there is unlikely to be simple string-based method of excluding filler strings such as this.\n",
    "\n",
    "As shown above, the `conll2002` Dutch corpus contains not just named entity annotation but also part-of-speech tags. This allows us to devise patterns that are sensitive to these tags, as shown in the next example. The method `clause()` prints out the relations in a clausal form, where the binary relation symbol is specified as the value of parameter `relsym`."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from nltk.corpus import conll2002\n",
    "vnv = \"\"\"\n",
    "(\n",
    "is/V|    # 3rd sing present and\n",
    "was/V|   # past forms of the verb zijn ('be')\n",
    "werd/V|  # and also present\n",
    "wordt/V  # past of worden ('become)\n",
    ")\n",
    ".*       # followed by anything\n",
    "van/Prep # followed by van ('of')\n",
    "\"\"\"\n",
    "VAN = re.compile(vnv, re.VERBOSE)\n",
    "for doc in conll2002.chunked_sents('ned.train'):\n",
    "    for rel in nltk.sem.extract_rels('PER', 'ORG', doc,\n",
    "                                   corpus='conll2002', pattern=VAN):\n",
    "        print(nltk.sem.clause(rel, relsym=\"VAN\")) \n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 7   Summary\n",
    "\n",
    "-    Information extraction systems search large bodies of unrestricted text for specific types of entities and relations, and use them to populate well-organized databases. These databases can then be used to find answers for specific questions.\n",
    "-    The typical architecture for an information extraction system begins by segmenting, tokenizing, and part-of-speech tagging the text. The resulting data is then searched for specific types of entity. Finally, the information extraction system looks at entities that are mentioned near one another in the text, and tries to determine whether specific relationships hold between those entities.\n",
    "-    Entity recognition is often performed using chunkers, which segment multi-token sequences, and label them with the appropriate entity type. Common entity types include ORGANIZATION, PERSON, LOCATION, DATE, TIME, MONEY, and GPE (geo-political entity).\n",
    "-    Chunkers can be constructed using rule-based systems, such as the `RegexpParser` class provided by NLTK; or using machine learning techniques, such as the `ConsecutiveNPChunker` presented in this chapter. In either case, part-of-speech tags are often a very important feature when searching for chunks.\n",
    "-    Although chunkers are specialized to create relatively flat data structures, where no two chunks are allowed to overlap, they can be cascaded together to build nested structures.\n",
    "-    Relation extraction can be performed using either rule-based systems which typically look for specific patterns in the text that connect entities and the intervening words; or using machine-learning systems which typically attempt to learn such patterns automatically from a training corpus.\n",
    "\n"
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.7 64-bit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "interpreter": {
   "hash": "7691dc7e968cae2c1305b7cd748cf3c51f27e2f981106cfee5b02ab727978e13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}